{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chU4cUFvmDUb",
    "outputId": "09dfbed7-f62a-49b7-d6fa-e24a53f4b519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Memory: 85.1 GB\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Project directory: /content/drive/MyDrive/AI574_Multi_Domain_Agent\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Create a persistent project directory\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/AI574_Multi_Domain_Agent'\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/data', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/models', exist_ok=True)\n",
    "print(f'Project directory: {PROJECT_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kH_erhJWpVc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m133.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Action: Run this cell to install all packages:\n",
    "\n",
    "# %%capture\n",
    "!pip install -q \\\n",
    "  \"langchain>=0.3.0\" \"langchain-core>=0.3.0\" \"langgraph>=0.2.0\" \"langchain-text-splitters>=0.2.0\" \\\n",
    "  \"keras>=3.0\" \"keras-hub>=0.17.0\" \\\n",
    "  \"sentence-transformers>=3.0.0\" \"transformers>=4.45,<5.0\" \"chromadb>=0.5.0\" \\\n",
    "  \"PyPDF2>=3.0.0\" \"arxiv>=2.1.0\" \"pydantic>=2.0\"\n",
    "\n",
    "!pip install -q \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTt0KS_jqRiY",
    "outputId": "f7c352a5-98b9-4278-acd1-33583fd75aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras backend: jax\n",
      "Keras version: 3.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "\n",
    "# Verify\n",
    "import keras\n",
    "print(f'Keras backend: {keras.backend.backend()}')\n",
    "print(f'Keras version: {keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPPKReZZquJR",
    "outputId": "793591bd-a513-4f9b-faa1-c402e002b1c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "JAX devices: [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import langgraph\n",
    "import keras_hub\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import jax\n",
    "print('All imports successful!')\n",
    "print(f'JAX devices: {jax.devices()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EhvPnZR8q8JX",
    "outputId": "799e0336-8a16-421f-fb91-44cf75bfff8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project structure created at: /content/drive/MyDrive/AI574_Multi_Domain_Agent/project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# PROJECT_DIR is already defined\n",
    "PROJECT_ROOT = os.path.join(PROJECT_DIR, 'project')\n",
    "\n",
    "dirs = [\n",
    "    'config',\n",
    "    'foundation',\n",
    "    'ingestion',\n",
    "    'rag_core',\n",
    "    'agents',\n",
    "    'orchestration',\n",
    "    'evaluation',\n",
    "    'data/industrial',\n",
    "    'data/recipes',\n",
    "]\n",
    "\n",
    "# Create directories\n",
    "for d in dirs:\n",
    "    os.makedirs(os.path.join(PROJECT_ROOT, d), exist_ok=True)\n",
    "\n",
    "# Create __init__.py in every package directory\n",
    "packages = [\n",
    "    'config',\n",
    "    'foundation',\n",
    "    'ingestion',\n",
    "    'rag_core',\n",
    "    'agents',\n",
    "    'orchestration',\n",
    "    'evaluation'\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    init_file = os.path.join(PROJECT_ROOT, pkg, '__init__.py')\n",
    "    open(init_file, 'a').close()\n",
    "\n",
    "# Add project root to Python path\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print('Project structure created at:', PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rC74O-8l39hm",
    "outputId": "f636bce9-3ff5-469f-a7d8-af577f9ed92e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = /content/drive/MyDrive/AI574_Multi_Domain_Agent/project\n",
      "foundation exists? True\n",
      "init exists? True\n",
      "files: ['embedding_service.py', 'vector_store.py', '__init__.py', '__pycache__', 'llm_wrapper.py']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# PROJECT_DIR already defined elsewhere\n",
    "PROJECT_ROOT = os.path.join(PROJECT_DIR, \"project\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "print(\"foundation exists?\", os.path.isdir(os.path.join(PROJECT_ROOT, \"foundation\")))\n",
    "print(\"init exists?\", os.path.isfile(os.path.join(PROJECT_ROOT, \"foundation\", \"__init__.py\")))\n",
    "print(\"files:\", os.listdir(os.path.join(PROJECT_ROOT, \"foundation\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Sync: write local source files to PROJECT_ROOT on Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# This cell embeds all .py files from your local project.\n",
    "# Run it to write them to Drive so Colab uses the latest code.\n",
    "# To update: edit files locally in Cursor, then re-run the sync script.\n",
    "\n",
    "import os, sys\n",
    "\n",
    "_FILES = {\n",
    "\n",
    "    'agents/__init__.py': r'''\n",
    "''',\n",
    "\n",
    "    'agents/base_agent.py': r'''\n",
    "\"\"\"\n",
    "Base Agent\n",
    "==========\n",
    "Abstract base class for all specialized agents.\n",
    "Each agent = domain config + optional preprocessing + shared CRAG pipeline.\n",
    "\n",
    "Subclasses only need to override:\n",
    "- domain (str)\n",
    "- preprocess_query() â€” optional domain-specific query prep\n",
    "- postprocess_response() â€” optional domain-specific formatting\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional\n",
    "\n",
    "from rag_core.crag_pipeline import CRAGPipeline, CRAGResult\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BaseAgent(ABC):\n",
    "    \"\"\"Abstract base for domain-specialized agents.\"\"\"\n",
    "\n",
    "    def __init__(self, crag_pipeline: CRAGPipeline):\n",
    "        self.crag = crag_pipeline\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def domain(self) -> str:\n",
    "        \"\"\"Domain identifier (matches vector store collection key).\"\"\"\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def description(self) -> str:\n",
    "        \"\"\"Human-readable description of this agent's capabilities.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def handle(self, query: str, **kwargs) -> CRAGResult:\n",
    "        \"\"\"\n",
    "        Full agent execution: preprocess â†’ CRAG â†’ postprocess.\n",
    "\n",
    "        This is the main entry point called by the supervisor.\n",
    "        \"\"\"\n",
    "        logger.info(f\"[{self.domain}] Handling query: {query[:80]}...\")\n",
    "\n",
    "        # Domain-specific query preprocessing\n",
    "        processed_query = self.preprocess_query(query)\n",
    "\n",
    "        # Run shared CRAG pipeline\n",
    "        result = self.crag.run(\n",
    "            query=processed_query,\n",
    "            domain=self.domain,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Domain-specific postprocessing\n",
    "        result = self.postprocess_response(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def preprocess_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Optional domain-specific query preprocessing.\n",
    "        Override in subclass to add terminology expansion, etc.\n",
    "        Default: pass-through.\n",
    "        \"\"\"\n",
    "        return query\n",
    "\n",
    "    def postprocess_response(self, result: CRAGResult) -> CRAGResult:\n",
    "        \"\"\"\n",
    "        Optional domain-specific response postprocessing.\n",
    "        Override in subclass to format output, add warnings, etc.\n",
    "        Default: pass-through.\n",
    "        \"\"\"\n",
    "        return result\n",
    "''',\n",
    "\n",
    "    'agents/industrial_agent.py': r'''\n",
    "\"\"\"\n",
    "Industrial Troubleshooting Agent\n",
    "================================\n",
    "Specialized for PLC/SCADA diagnostics, fault code interpretation,\n",
    "and maintenance procedures. Adds industrial-specific query preprocessing\n",
    "and safety warnings to responses.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from agents.base_agent import BaseAgent\n",
    "from rag_core.crag_pipeline import CRAGPipeline, CRAGResult\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Known fault code patterns by vendor\n",
    "FAULT_CODE_PATTERNS = {\n",
    "    \"siemens\": re.compile(r'\\b[Ff]\\d{4,5}\\b'),           # F0003, F07900\n",
    "    \"allen_bradley\": re.compile(r'\\bF\\d{1,3}\\b'),         # F2, F33\n",
    "    \"generic\": re.compile(r'\\b(?:fault|error|alarm)\\s*(?:code\\s*)?[:#]?\\s*\\d+', re.I),\n",
    "}\n",
    "\n",
    "# Equipment model patterns\n",
    "EQUIPMENT_PATTERNS = re.compile(\n",
    "    r'\\b(S7-\\d{3,4}|1[257]\\d{2}|6ES\\d|PanelView|PowerFlex\\s*\\d+)\\b', re.I\n",
    ")\n",
    "\n",
    "\n",
    "class IndustrialAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Industrial Equipment Troubleshooting Specialist.\n",
    "\n",
    "    Enhancements over base agent:\n",
    "    - Extracts and normalizes fault codes from queries\n",
    "    - Expands industrial abbreviations\n",
    "    - Adds safety warnings to responses involving electrical/mechanical work\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def domain(self) -> str:\n",
    "        return \"industrial\"\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return (\n",
    "            \"Industrial equipment troubleshooting specialist. Handles PLC/SCADA \"\n",
    "            \"diagnostics, fault code interpretation, motor drives, industrial \"\n",
    "            \"networking, and maintenance procedures.\"\n",
    "        )\n",
    "\n",
    "    def preprocess_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Industrial-specific query preprocessing:\n",
    "        1. Extract and normalize fault codes\n",
    "        2. Identify equipment models\n",
    "        3. Expand common abbreviations for better retrieval\n",
    "        \"\"\"\n",
    "        processed = query\n",
    "\n",
    "        # Extract fault codes and make them explicit\n",
    "        for vendor, pattern in FAULT_CODE_PATTERNS.items():\n",
    "            matches = pattern.findall(processed)\n",
    "            if matches:\n",
    "                # Ensure fault code is prominent for retrieval\n",
    "                codes = \", \".join(matches)\n",
    "                if \"fault\" not in processed.lower():\n",
    "                    processed += f\" (fault code: {codes})\"\n",
    "                logger.debug(f\"Extracted fault codes ({vendor}): {codes}\")\n",
    "\n",
    "        # Extract equipment models\n",
    "        equip_matches = EQUIPMENT_PATTERNS.findall(processed)\n",
    "        if equip_matches:\n",
    "            logger.debug(f\"Detected equipment: {equip_matches}\")\n",
    "\n",
    "        return processed\n",
    "\n",
    "    def postprocess_response(self, result: CRAGResult) -> CRAGResult:\n",
    "        \"\"\"\n",
    "        Add safety warnings for responses involving potentially\n",
    "        dangerous procedures (electrical, mechanical, pressurized systems).\n",
    "        \"\"\"\n",
    "        safety_keywords = [\n",
    "            \"high voltage\", \"lockout\", \"tagout\", \"loto\",\n",
    "            \"energized\", \"live wire\", \"arc flash\",\n",
    "            \"pressurized\", \"hydraulic\", \"pneumatic\",\n",
    "            \"rotating\", \"pinch point\", \"confined space\",\n",
    "        ]\n",
    "\n",
    "        response_lower = result.response.lower()\n",
    "        triggered_warnings = [\n",
    "            kw for kw in safety_keywords if kw in response_lower\n",
    "        ]\n",
    "\n",
    "        if triggered_warnings:\n",
    "            safety_notice = (\n",
    "                \"\\n\\nâš ï¸ **SAFETY WARNING**: This procedure may involve \"\n",
    "                \"hazardous conditions. Ensure all applicable safety protocols \"\n",
    "                \"(lockout/tagout, PPE, permits) are followed. Consult your \"\n",
    "                \"site safety officer before proceeding.\"\n",
    "            )\n",
    "            result.response += safety_notice\n",
    "            logger.info(f\"Safety warning added (triggers: {triggered_warnings})\")\n",
    "\n",
    "        return result\n",
    "''',\n",
    "\n",
    "    'agents/recipe_agent.py': r'''\n",
    "\"\"\"\n",
    "Recipe Assistant Agent\n",
    "======================\n",
    "Specialized for cooking guidance, ingredient substitutions,\n",
    "and recipe recommendations. Adds ingredient-aware query preprocessing\n",
    "and nutrition highlighting.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from agents.base_agent import BaseAgent\n",
    "from rag_core.crag_pipeline import CRAGPipeline, CRAGResult\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Common substitution triggers\n",
    "SUBSTITUTION_KEYWORDS = [\n",
    "    \"substitute\", \"replace\", \"instead of\", \"alternative\",\n",
    "    \"swap\", \"without\", \"allergy\", \"intolerant\", \"vegan\",\n",
    "    \"dairy-free\", \"gluten-free\", \"no eggs\",\n",
    "]\n",
    "\n",
    "# Cooking technique synonyms for query expansion\n",
    "TECHNIQUE_SYNONYMS = {\n",
    "    \"fry\": [\"sautÃ©\", \"pan-fry\", \"deep-fry\", \"stir-fry\"],\n",
    "    \"bake\": [\"roast\", \"oven\"],\n",
    "    \"boil\": [\"simmer\", \"poach\", \"blanch\"],\n",
    "    \"grill\": [\"broil\", \"char-grill\", \"barbecue\"],\n",
    "}\n",
    "\n",
    "\n",
    "class RecipeAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Recipe Assistant Specialist.\n",
    "\n",
    "    Enhancements over base agent:\n",
    "    - Detects substitution queries and adds context\n",
    "    - Expands cooking technique terms for broader retrieval\n",
    "    - Tags dietary information in responses\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def domain(self) -> str:\n",
    "        return \"recipe\"\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return (\n",
    "            \"Recipe assistant for cooking guidance, ingredient substitutions, \"\n",
    "            \"recipe search, nutritional information, and food preparation techniques.\"\n",
    "        )\n",
    "\n",
    "    def preprocess_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Recipe-specific query preprocessing:\n",
    "        1. Detect substitution intent and enrich query\n",
    "        2. Expand cooking technique terms\n",
    "        \"\"\"\n",
    "        processed = query\n",
    "        query_lower = query.lower()\n",
    "\n",
    "        # Check for substitution queries\n",
    "        is_substitution = any(kw in query_lower for kw in SUBSTITUTION_KEYWORDS)\n",
    "        if is_substitution:\n",
    "            # Add \"substitution\" explicitly for retrieval boost\n",
    "            if \"substitut\" not in query_lower:\n",
    "                processed += \" substitution alternative\"\n",
    "            logger.debug(\"Detected substitution query\")\n",
    "\n",
    "        # Expand cooking techniques\n",
    "        for technique, synonyms in TECHNIQUE_SYNONYMS.items():\n",
    "            if technique in query_lower:\n",
    "                expansion = \" \".join(synonyms[:2])  # Add top 2 synonyms\n",
    "                processed += f\" {expansion}\"\n",
    "                logger.debug(f\"Expanded technique '{technique}' with: {expansion}\")\n",
    "                break\n",
    "\n",
    "        return processed\n",
    "\n",
    "    def postprocess_response(self, result: CRAGResult) -> CRAGResult:\n",
    "        \"\"\"\n",
    "        Add dietary tags if relevant dietary terms are detected\n",
    "        in the response.\n",
    "        \"\"\"\n",
    "        dietary_tags = {\n",
    "            \"vegan\": [\"vegan\", \"plant-based\"],\n",
    "            \"vegetarian\": [\"vegetarian\", \"no meat\"],\n",
    "            \"gluten-free\": [\"gluten-free\", \"celiac\"],\n",
    "            \"dairy-free\": [\"dairy-free\", \"lactose\"],\n",
    "            \"nut-free\": [\"nut-free\", \"nut allergy\"],\n",
    "        }\n",
    "\n",
    "        response_lower = result.response.lower()\n",
    "        detected_tags = [\n",
    "            tag for tag, keywords in dietary_tags.items()\n",
    "            if any(kw in response_lower for kw in keywords)\n",
    "        ]\n",
    "\n",
    "        if detected_tags:\n",
    "            tags_str = \" | \".join(f\"ğŸ·ï¸ {tag}\" for tag in detected_tags)\n",
    "            result.response += f\"\\n\\n{tags_str}\"\n",
    "\n",
    "        return result\n",
    "''',\n",
    "\n",
    "    'agents/scientific_agent.py': r'''\n",
    "\"\"\"\n",
    "Scientific Paper Summarizer Agent\n",
    "==================================\n",
    "Specialized for academic literature synthesis via ArXiv.\n",
    "Supports on-demand paper retrieval and citation-aware summarization.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "from agents.base_agent import BaseAgent\n",
    "from rag_core.crag_pipeline import CRAGPipeline, CRAGResult\n",
    "from ingestion.index_builder import IndexBuilder\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ScientificAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Scientific Paper Summarization Specialist.\n",
    "\n",
    "    Enhancements over base agent:\n",
    "    - On-demand ArXiv fetching when local index is insufficient\n",
    "    - Adds citation formatting to responses\n",
    "    - Structures summaries as Objective â†’ Method â†’ Findings â†’ Limitations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        crag_pipeline: CRAGPipeline,\n",
    "        index_builder: Optional[IndexBuilder] = None,\n",
    "    ):\n",
    "        super().__init__(crag_pipeline)\n",
    "        self.index_builder = index_builder\n",
    "\n",
    "    @property\n",
    "    def domain(self) -> str:\n",
    "        return \"scientific\"\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return (\n",
    "            \"Scientific paper summarization specialist. Handles ArXiv research \"\n",
    "            \"queries, literature synthesis, and academic concept explanation.\"\n",
    "        )\n",
    "\n",
    "    def handle(self, query: str, **kwargs) -> CRAGResult:\n",
    "        \"\"\"\n",
    "        Extended handle: if local retrieval fails AND we have an index builder,\n",
    "        fetch fresh papers from ArXiv and retry.\n",
    "        \"\"\"\n",
    "        # First, try with existing indexed papers\n",
    "        result = super().handle(query, **kwargs)\n",
    "\n",
    "        # If escalated due to no relevant docs, try fetching from ArXiv\n",
    "        if (\n",
    "            result.escalated\n",
    "            and self.index_builder\n",
    "            and \"No documents\" in result.escalation_reason\n",
    "        ):\n",
    "            logger.info(\"Local retrieval failed â€” fetching from ArXiv on-demand\")\n",
    "            try:\n",
    "                count = self.index_builder.index_arxiv_papers(\n",
    "                    query, max_results=10\n",
    "                )\n",
    "                if count > 0:\n",
    "                    logger.info(f\"Indexed {count} new ArXiv papers, retrying...\")\n",
    "                    result = super().handle(query, **kwargs)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"ArXiv on-demand fetch failed: {e}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def preprocess_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Scientific-specific query preprocessing:\n",
    "        - Strip conversational fluff for better retrieval\n",
    "        - Add \"research paper\" context if not present\n",
    "        \"\"\"\n",
    "        # Remove conversational prefixes\n",
    "        strip_prefixes = [\n",
    "            \"can you find\", \"please summarize\", \"i want to know about\",\n",
    "            \"tell me about\", \"search for\", \"find papers on\",\n",
    "        ]\n",
    "        processed = query\n",
    "        query_lower = query.lower()\n",
    "        for prefix in strip_prefixes:\n",
    "            if query_lower.startswith(prefix):\n",
    "                processed = query[len(prefix):].strip()\n",
    "                break\n",
    "\n",
    "        return processed\n",
    "\n",
    "    def postprocess_response(self, result: CRAGResult) -> CRAGResult:\n",
    "        \"\"\"\n",
    "        Add citation block at the end of response if sources have ArXiv IDs.\n",
    "        \"\"\"\n",
    "        arxiv_sources = [\n",
    "            s for s in result.sources\n",
    "            if \"arxiv\" in s.get(\"source\", \"\").lower()\n",
    "        ]\n",
    "\n",
    "        if arxiv_sources:\n",
    "            citations = []\n",
    "            for i, src in enumerate(arxiv_sources, 1):\n",
    "                parent_id = src.get(\"parent_id\", \"\")\n",
    "                citations.append(f\"  [{i}] {parent_id}\")\n",
    "\n",
    "            if citations:\n",
    "                result.response += \"\\n\\nğŸ“š **References:**\\n\" + \"\\n\".join(citations)\n",
    "\n",
    "        return result\n",
    "''',\n",
    "\n",
    "    'config/__init__.py': r'''\n",
    "''',\n",
    "\n",
    "    'config/prompts.py': r'''\n",
    "\"\"\"\n",
    "Prompt templates for all agents.\n",
    "Stored as Python constants for Colab compatibility (no YAML dependency needed).\n",
    "Each prompt uses {placeholders} for runtime injection.\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€ Supervisor / Router â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "SUPERVISOR_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are the Supervisor Agent of a Multi-Domain Intelligent Assistant.\n",
    "Your job is to analyze user queries and route them to the correct specialist agent.\n",
    "\n",
    "Available specialists:\n",
    "1. **industrial** â€” Industrial equipment troubleshooting, PLC/SCADA systems, \\\n",
    "fault codes, maintenance procedures, automation diagnostics.\n",
    "2. **recipe** â€” Cooking guidance, recipe search, ingredient substitutions, \\\n",
    "nutritional information, food preparation techniques.\n",
    "3. **scientific** â€” Scientific paper summarization, ArXiv research queries, \\\n",
    "literature synthesis, academic concept explanation.\n",
    "\n",
    "Routing rules:\n",
    "- Classify the user's intent and select EXACTLY ONE specialist.\n",
    "- If the query clearly belongs to one domain, route with high confidence.\n",
    "- If the query is ambiguous or spans multiple domains, ask for clarification.\n",
    "- If the query doesn't match any domain, route to \"fallback\" for web search.\n",
    "\n",
    "Respond ONLY with valid JSON:\n",
    "{{\n",
    "  \"domain\": \"<industrial|recipe|scientific|clarify|fallback>\",\n",
    "  \"confidence\": <0.0-1.0>,\n",
    "  \"reasoning\": \"<brief explanation of routing decision>\"\n",
    "}}\n",
    "Reply with only that JSON object, on a single line if possible, with no explanation or prefix.\n",
    "\n",
    "Few-shot examples:\n",
    "\n",
    "User: \"My Siemens S7-1200 PLC is showing fault code F0003\"\n",
    "{{\"domain\": \"industrial\", \"confidence\": 0.98, \"reasoning\": \"PLC fault code troubleshooting\"}}\n",
    "\n",
    "User: \"What can I substitute for buttermilk in a pancake recipe?\"\n",
    "{{\"domain\": \"recipe\", \"confidence\": 0.95, \"reasoning\": \"Ingredient substitution query\"}}\n",
    "\n",
    "User: \"Summarize recent transformer architecture papers on ArXiv\"\n",
    "{{\"domain\": \"scientific\", \"confidence\": 0.96, \"reasoning\": \"Scientific literature request\"}}\n",
    "\n",
    "User: \"What temperature should I cook chicken to avoid equipment failure?\"\n",
    "{{\"domain\": \"clarify\", \"confidence\": 0.40, \"reasoning\": \"Ambiguous â€” could be cooking or industrial equipment\"}}\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€ Industrial Troubleshooting Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "INDUSTRIAL_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are an Industrial Equipment Troubleshooting Specialist with expertise in:\n",
    "- PLC programming and diagnostics (Siemens, Allen-Bradley, Mitsubishi)\n",
    "- SCADA systems and HMI interfaces\n",
    "- Motor drives, VFDs, and servo systems\n",
    "- Industrial networking (PROFINET, EtherNet/IP, Modbus)\n",
    "- Preventive and predictive maintenance procedures\n",
    "- Fault code interpretation and resolution\n",
    "\n",
    "Instructions:\n",
    "1. Base your answers ONLY on the retrieved context documents provided below.\n",
    "2. If the context is insufficient, say so explicitly â€” do not guess or hallucinate.\n",
    "3. Always reference specific document sections, fault codes, or procedure numbers.\n",
    "4. Use clear step-by-step troubleshooting format when applicable.\n",
    "5. Flag any safety warnings prominently.\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "User Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€ Recipe Assistant Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "RECIPE_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a Recipe Assistant with expertise in cooking guidance, ingredient \\\n",
    "knowledge, and nutritional information.\n",
    "\n",
    "Instructions:\n",
    "1. Base your answers on the retrieved recipe context provided below.\n",
    "2. When recommending recipes, mention key ingredients, prep time, and difficulty.\n",
    "3. For substitution queries, explain WHY the substitute works (chemistry/texture).\n",
    "4. Include relevant nutritional highlights when available.\n",
    "5. If the context doesn't contain a suitable answer, say so clearly.\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "User Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€ Scientific Summarizer Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "SCIENTIFIC_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a Scientific Paper Summarization Specialist. You synthesize academic \\\n",
    "research into clear, accurate summaries.\n",
    "\n",
    "Instructions:\n",
    "1. Base your summaries ONLY on the retrieved paper content below.\n",
    "2. Structure summaries as: Objective â†’ Method â†’ Key Findings â†’ Limitations.\n",
    "3. When synthesizing multiple papers, identify agreements and contradictions.\n",
    "4. Preserve technical accuracy â€” do not oversimplify domain-specific terms.\n",
    "5. Always include citation information (authors, year, ArXiv ID).\n",
    "6. If context is insufficient for a complete answer, state what's missing.\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "User Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€ Document Grader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "GRADER_PROMPT = \"\"\"\\\n",
    "You are a document relevance grader. Given a user query and a retrieved document, \\\n",
    "assess whether the document is relevant to answering the query.\n",
    "\n",
    "Respond with ONLY valid JSON:\n",
    "{{\n",
    "  \"relevance\": \"<relevant|irrelevant|ambiguous>\",\n",
    "  \"score\": <0.0-1.0>,\n",
    "  \"reasoning\": \"<brief explanation>\"\n",
    "}}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Retrieved Document:\n",
    "{document}\n",
    "\"\"\"\n",
    "\n",
    "# Batch version: grade all documents in one call (much faster than N separate calls)\n",
    "GRADER_BATCH_PROMPT = \"\"\"\\\n",
    "You are a document relevance grader. Given a user query and several retrieved documents, \\\n",
    "assess each document's relevance to answering the query.\n",
    "\n",
    "Respond with ONLY valid JSON. One object per document, in order (doc_0, doc_1, ...):\n",
    "{{\n",
    "  \"grades\": [\n",
    "    {{ \"relevance\": \"<relevant|irrelevant|ambiguous>\", \"score\": <0.0-1.0>, \"reasoning\": \"<brief>\" }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Documents (numbered):\n",
    "{documents_block}\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€ Query Rewriter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "REWRITER_PROMPT = \"\"\"\\\n",
    "You are a query rewriting specialist. The original query failed to retrieve \\\n",
    "relevant documents. Rewrite it to improve retrieval while preserving the \\\n",
    "user's intent.\n",
    "\n",
    "Strategies:\n",
    "- Add domain-specific synonyms or technical terms\n",
    "- Expand abbreviations\n",
    "- Decompose compound questions into focused sub-queries\n",
    "- Remove ambiguous phrasing\n",
    "\n",
    "Original query: {query}\n",
    "Domain: {domain}\n",
    "Previous failed attempt context: {failure_context}\n",
    "\n",
    "Respond with ONLY the rewritten query text, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€ Hallucination Checker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "HALLUCINATION_CHECK_SYSTEM = \"\"\"\\\n",
    "You are a hallucination detector.  Analyze a response against its source \\\n",
    "documents and decide whether every claim is grounded.\n",
    "\n",
    "Respond with ONLY valid JSON â€” no other text:\n",
    "{{\"grounded\": true/false, \"confidence\": 0.0-1.0, \"issues\": [\"...\"]}}\n",
    "\"\"\"\n",
    "\n",
    "HALLUCINATION_CHECK_USER = \"\"\"\\\n",
    "Source Documents:\n",
    "{sources}\n",
    "\n",
    "Generated Response:\n",
    "{response}\n",
    "\n",
    "Return ONLY JSON.\"\"\"\n",
    "''',\n",
    "\n",
    "    'config/settings.py': r'''\n",
    "\"\"\"\n",
    "Central configuration for the Multi-Domain Intelligent Assistant.\n",
    "All model paths, hyperparameters, and constants live here.\n",
    "Adjust for your Colab environment (GPU type, memory, etc.).\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# â”€â”€ Model Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "@dataclass\n",
    "class LLMConfig:\n",
    "    \"\"\"Configuration for the primary language model.\"\"\"\n",
    "    preset: str = \"gemma3_instruct_12b\"             # KerasHub preset (~24GB bfloat16, fits A100 80GB)\n",
    "    backend: str = \"jax\"                             # jax | torch | tensorflow\n",
    "    dtype: str = \"bfloat16\"                          # bfloat16 for A100/L4\n",
    "    max_new_tokens: int = 1024                       # full-generation budget (answers)\n",
    "    short_max_new_tokens: int = 256                  # budget for routing, grading, hallucination checks\n",
    "    temperature: float = 0.7\n",
    "    top_k: int = 50\n",
    "    top_p: float = 0.9\n",
    "    fallback_preset: Optional[str] = None             # None = no fallback (avoids double OOM)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    \"\"\"Configuration for the embedding model.\"\"\"\n",
    "    model_name: str = \"thenlper/gte-large\"\n",
    "    dimension: int = 1024\n",
    "    batch_size: int = 64\n",
    "    max_seq_length: int = 512\n",
    "    device: str = \"cuda\"\n",
    "    trust_remote_code: bool = False\n",
    "\n",
    "\n",
    "# â”€â”€ Vector Store Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "@dataclass\n",
    "class VectorStoreConfig:\n",
    "    \"\"\"Configuration for ChromaDB vector store.\"\"\"\n",
    "    persist_directory: str = \"./chroma_db\"\n",
    "    collections: dict = field(default_factory=lambda: {\n",
    "        \"industrial\": \"industrial_knowledge\",\n",
    "        \"recipe\": \"recipe_knowledge\",\n",
    "        \"scientific\": \"scientific_knowledge\",\n",
    "    })\n",
    "    search_top_k: int = 5\n",
    "    similarity_metric: str = \"cosine\"                # cosine | l2 | ip\n",
    "\n",
    "\n",
    "# â”€â”€ Chunking Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "@dataclass\n",
    "class ChunkingConfig:\n",
    "    \"\"\"Configuration for document chunking.\"\"\"\n",
    "    chunk_size: int = 768                            # tokens (target midpoint)\n",
    "    chunk_overlap_pct: float = 0.15                  # 15% overlap\n",
    "    min_chunk_size: int = 512\n",
    "    max_chunk_size: int = 1024\n",
    "    separators: list = field(default_factory=lambda: [\n",
    "        \"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \"\n",
    "    ])\n",
    "\n",
    "\n",
    "# â”€â”€ RAG / CRAG Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for the Corrective RAG pipeline.\"\"\"\n",
    "    max_rewrite_attempts: int = 2\n",
    "    relevance_threshold: float = 0.7                 # doc grading cutoff\n",
    "    confidence_threshold: float = 0.6                # hallucination check\n",
    "    grading_labels: tuple = (\"relevant\", \"irrelevant\", \"ambiguous\")\n",
    "    skip_hallucination_check: bool = False            # True = skip validation (saves one LLM call per query)\n",
    "\n",
    "\n",
    "# â”€â”€ Supervisor / Routing Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "@dataclass\n",
    "class DomainSpec:\n",
    "    \"\"\"Specification for a single domain (used for config-driven wiring).\"\"\"\n",
    "    name: str\n",
    "    agent_class: str          # dotted import path, e.g. \"agents.industrial_agent.IndustrialAgent\"\n",
    "    prompt_key: str           # attribute name in config.prompts, e.g. \"INDUSTRIAL_SYSTEM_PROMPT\"\n",
    "    collection: str           # vector store collection name\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SupervisorConfig:\n",
    "    \"\"\"Configuration for the supervisor/router agent.\"\"\"\n",
    "    routing_confidence_threshold: float = 0.75       # below â†’ clarify\n",
    "    domains: list = field(default_factory=lambda: [\n",
    "        \"industrial\", \"recipe\", \"scientific\"\n",
    "    ])\n",
    "    domain_registry: list = field(default_factory=lambda: [\n",
    "        DomainSpec(\"industrial\", \"agents.industrial_agent.IndustrialAgent\",\n",
    "                   \"INDUSTRIAL_SYSTEM_PROMPT\", \"industrial_knowledge\"),\n",
    "        DomainSpec(\"recipe\", \"agents.recipe_agent.RecipeAgent\",\n",
    "                   \"RECIPE_SYSTEM_PROMPT\", \"recipe_knowledge\"),\n",
    "        DomainSpec(\"scientific\", \"agents.scientific_agent.ScientificAgent\",\n",
    "                   \"SCIENTIFIC_SYSTEM_PROMPT\", \"scientific_knowledge\"),\n",
    "    ])\n",
    "    fallback_tool: str = \"tavily_search\"\n",
    "\n",
    "\n",
    "# â”€â”€ Aggregate Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "@dataclass\n",
    "class ProjectConfig:\n",
    "    \"\"\"Top-level configuration combining all sub-configs.\"\"\"\n",
    "    llm: LLMConfig = field(default_factory=LLMConfig)\n",
    "    embedding: EmbeddingConfig = field(default_factory=EmbeddingConfig)\n",
    "    vector_store: VectorStoreConfig = field(default_factory=VectorStoreConfig)\n",
    "    chunking: ChunkingConfig = field(default_factory=ChunkingConfig)\n",
    "    rag: RAGConfig = field(default_factory=RAGConfig)\n",
    "    supervisor: SupervisorConfig = field(default_factory=SupervisorConfig)\n",
    "\n",
    "\n",
    "# Default config instance â€” import this everywhere\n",
    "CONFIG = ProjectConfig()\n",
    "''',\n",
    "\n",
    "    'evaluation/__init__.py': r'''\n",
    "''',\n",
    "\n",
    "    'evaluation/metrics.py': r'''\n",
    "\"\"\"\n",
    "Evaluation Framework\n",
    "====================\n",
    "Test suite, metrics, and LLM-judge for evaluating the multi-agent system.\n",
    "Covers: routing accuracy, retrieval precision, task completion, and\n",
    "self-correction effectiveness.\n",
    "\n",
    "Usage:\n",
    "    from evaluation.metrics import Evaluator\n",
    "    evaluator = Evaluator(workflow=workflow)\n",
    "    results = evaluator.run_full_evaluation()\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# â”€â”€ Test Suite â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 150 test queries: 50 per domain\n",
    "# Each entry: (query, expected_domain, difficulty, description)\n",
    "\n",
    "INDUSTRIAL_TEST_QUERIES = [\n",
    "    (\"My S7-1200 PLC is showing fault code F0003, what does it mean?\", \"industrial\", \"easy\", \"Direct fault code lookup\"),\n",
    "    (\"The VFD on pump station 3 is tripping on overcurrent every morning\", \"industrial\", \"medium\", \"Intermittent fault diagnosis\"),\n",
    "    (\"How do I configure PROFINET communication between two Siemens PLCs?\", \"industrial\", \"medium\", \"Configuration procedure\"),\n",
    "    (\"What are the recommended PM intervals for Allen-Bradley servo drives?\", \"industrial\", \"easy\", \"Maintenance schedule\"),\n",
    "    (\"Our SCADA system is losing connection to RTUs intermittently\", \"industrial\", \"hard\", \"Network troubleshooting\"),\n",
    "    (\"Explain the difference between SIL 2 and SIL 3 safety requirements\", \"industrial\", \"medium\", \"Safety standards\"),\n",
    "    (\"Motor bearing temperature is trending upward over the last month\", \"industrial\", \"medium\", \"Predictive maintenance\"),\n",
    "    (\"How to reset a PowerFlex 525 drive after an F0063 fault?\", \"industrial\", \"easy\", \"Fault recovery procedure\"),\n",
    "    (\"What causes ground fault alarms on a 480V distribution panel?\", \"industrial\", \"medium\", \"Electrical troubleshooting\"),\n",
    "    (\"Best practices for PLC program backup and version control\", \"industrial\", \"easy\", \"Best practices\"),\n",
    "    # ... Add 40 more for full test suite\n",
    "]\n",
    "\n",
    "RECIPE_TEST_QUERIES = [\n",
    "    (\"What can I substitute for eggs in a chocolate cake recipe?\", \"recipe\", \"easy\", \"Common substitution\"),\n",
    "    (\"I have chicken, rice, and bell peppers. What can I make?\", \"recipe\", \"medium\", \"Ingredient-based search\"),\n",
    "    (\"How do I make a proper roux for gumbo?\", \"recipe\", \"easy\", \"Technique question\"),\n",
    "    (\"What's the difference between baking soda and baking powder?\", \"recipe\", \"easy\", \"Ingredient knowledge\"),\n",
    "    (\"Give me a gluten-free pasta recipe with under 500 calories\", \"recipe\", \"medium\", \"Dietary constraint search\"),\n",
    "    (\"How long should I rest a steak after grilling?\", \"recipe\", \"easy\", \"Technique timing\"),\n",
    "    (\"What's a good dairy-free alternative to heavy cream in soup?\", \"recipe\", \"medium\", \"Dietary substitution\"),\n",
    "    (\"How do I properly temper chocolate for dipping?\", \"recipe\", \"hard\", \"Advanced technique\"),\n",
    "    (\"What spices pair well with lamb?\", \"recipe\", \"easy\", \"Flavor pairing\"),\n",
    "    (\"Can you give me a quick weeknight dinner recipe for 4 people?\", \"recipe\", \"medium\", \"Recommendation\"),\n",
    "    # ... Add 40 more for full test suite\n",
    "]\n",
    "\n",
    "SCIENTIFIC_TEST_QUERIES = [\n",
    "    (\"Summarize recent papers on transformer attention mechanisms\", \"scientific\", \"medium\", \"Topic synthesis\"),\n",
    "    (\"What are the key findings from the latest RLHF research?\", \"scientific\", \"medium\", \"Current research\"),\n",
    "    (\"Find papers about graph neural networks for drug discovery\", \"scientific\", \"easy\", \"Topic search\"),\n",
    "    (\"What is the state of the art in protein structure prediction?\", \"scientific\", \"hard\", \"SOTA review\"),\n",
    "    (\"Compare self-supervised vs supervised pretraining for NLP\", \"scientific\", \"hard\", \"Comparative analysis\"),\n",
    "    (\"What datasets are commonly used for named entity recognition?\", \"scientific\", \"easy\", \"Resource query\"),\n",
    "    (\"Explain the mixture of experts architecture in LLMs\", \"scientific\", \"medium\", \"Concept explanation\"),\n",
    "    (\"Recent advances in federated learning for medical imaging\", \"scientific\", \"medium\", \"Domain-specific search\"),\n",
    "    (\"What are the limitations of current RAG approaches?\", \"scientific\", \"medium\", \"Limitations analysis\"),\n",
    "    (\"Find papers on energy-efficient training of large language models\", \"scientific\", \"easy\", \"Topic search\"),\n",
    "    # ... Add 40 more for full test suite\n",
    "]\n",
    "\n",
    "# Ambiguous/edge-case queries for routing stress testing\n",
    "EDGE_CASE_QUERIES = [\n",
    "    (\"What temperature should I cook chicken to avoid equipment failure?\", \"clarify\", \"hard\", \"Ambiguous cross-domain\"),\n",
    "    (\"How do I calibrate my oven thermometer?\", \"clarify\", \"medium\", \"Could be recipe or industrial\"),\n",
    "    (\"What is the recipe for disaster recovery?\", \"clarify\", \"hard\", \"Metaphorical language\"),\n",
    "    (\"Tell me about mixing processes in chemical plants\", \"industrial\", \"medium\", \"Industrial â€” not recipe mixing\"),\n",
    "    (\"How does a neural network learn?\", \"scientific\", \"easy\", \"Clear scientific\"),\n",
    "]\n",
    "\n",
    "ALL_TEST_QUERIES = (\n",
    "    INDUSTRIAL_TEST_QUERIES\n",
    "    + RECIPE_TEST_QUERIES\n",
    "    + SCIENTIFIC_TEST_QUERIES\n",
    "    + EDGE_CASE_QUERIES\n",
    ")\n",
    "\n",
    "\n",
    "# â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Aggregated evaluation results.\"\"\"\n",
    "    routing_accuracy: float = 0.0\n",
    "    routing_by_domain: Dict[str, float] = field(default_factory=dict)\n",
    "    task_completion_rate: float = 0.0\n",
    "    avg_latency_seconds: float = 0.0\n",
    "    self_correction_rate: float = 0.0\n",
    "    per_query_results: List[Dict] = field(default_factory=list)\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Full evaluation suite for the multi-agent system.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, workflow, llm=None):\n",
    "        self.workflow = workflow\n",
    "        self.llm = llm  # For LLM-judge scoring\n",
    "\n",
    "    def evaluate_routing(\n",
    "        self,\n",
    "        test_queries: Optional[List] = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate routing accuracy across all test queries.\n",
    "\n",
    "        Returns accuracy overall and per-domain.\n",
    "        \"\"\"\n",
    "        from orchestration.workflow_graph import run_query\n",
    "\n",
    "        queries = test_queries or ALL_TEST_QUERIES\n",
    "        correct = 0\n",
    "        domain_correct = {}\n",
    "        domain_total = {}\n",
    "        results = []\n",
    "\n",
    "        for query, expected_domain, difficulty, desc in queries:\n",
    "            start = time.time()\n",
    "            result = run_query(self.workflow, query)\n",
    "            latency = time.time() - start\n",
    "\n",
    "            actual_domain = result.get(\"domain\", \"\")\n",
    "            is_correct = actual_domain == expected_domain\n",
    "\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "\n",
    "            domain_correct[expected_domain] = (\n",
    "                domain_correct.get(expected_domain, 0) + (1 if is_correct else 0)\n",
    "            )\n",
    "            domain_total[expected_domain] = (\n",
    "                domain_total.get(expected_domain, 0) + 1\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"expected\": expected_domain,\n",
    "                \"actual\": actual_domain,\n",
    "                \"correct\": is_correct,\n",
    "                \"confidence\": result.get(\"confidence\", 0),\n",
    "                \"latency\": latency,\n",
    "                \"difficulty\": difficulty,\n",
    "            })\n",
    "\n",
    "        accuracy = correct / len(queries) if queries else 0.0\n",
    "        per_domain = {\n",
    "            d: domain_correct.get(d, 0) / domain_total.get(d, 1)\n",
    "            for d in domain_total\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"overall_accuracy\": accuracy,\n",
    "            \"per_domain_accuracy\": per_domain,\n",
    "            \"total_queries\": len(queries),\n",
    "            \"correct\": correct,\n",
    "            \"results\": results,\n",
    "        }\n",
    "\n",
    "    def evaluate_llm_judge(\n",
    "        self,\n",
    "        query: str,\n",
    "        response: str,\n",
    "        domain: str,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Use LLM-as-judge to score a response for quality.\n",
    "\n",
    "        Scores on: relevance, accuracy, completeness, clarity (each 1-5).\n",
    "        \"\"\"\n",
    "        if not self.llm:\n",
    "            return {\"error\": \"No LLM provided for judging\"}\n",
    "\n",
    "        prompt = (\n",
    "            f\"You are evaluating an AI assistant's response quality.\\n\\n\"\n",
    "            f\"Domain: {domain}\\n\"\n",
    "            f\"User Query: {query}\\n\"\n",
    "            f\"Assistant Response: {response}\\n\\n\"\n",
    "            f\"Score the response on these dimensions (1-5 each):\\n\"\n",
    "            f\"1. Relevance â€” Does it address the query?\\n\"\n",
    "            f\"2. Accuracy â€” Is the information correct and grounded?\\n\"\n",
    "            f\"3. Completeness â€” Does it fully answer the question?\\n\"\n",
    "            f\"4. Clarity â€” Is it well-organized and easy to understand?\\n\\n\"\n",
    "            f\"Respond with ONLY valid JSON:\\n\"\n",
    "            f'{{\"relevance\": <1-5>, \"accuracy\": <1-5>, '\n",
    "            f'\"completeness\": <1-5>, \"clarity\": <1-5>, '\n",
    "            f'\"overall\": <1-5>, \"feedback\": \"<brief explanation>\"}}'\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = self.llm.invoke_and_parse_json(\n",
    "                [HumanMessage(content=prompt)]\n",
    "            )\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    # â”€â”€ Verification: Functional â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def verify_functional(self, queries: Optional[List] = None) -> Dict:\n",
    "        \"\"\"Run representative queries across all domains, clarify, and fallback.\n",
    "\n",
    "        Checks that every domain returns a non-empty response and a\n",
    "        valid confidence value, and that the timing dict has a stable schema.\n",
    "        \"\"\"\n",
    "        from orchestration.workflow_graph import run_query\n",
    "\n",
    "        queries = queries or [\n",
    "            (\"My PowerFlex 525 drive shows fault F004\", \"industrial\"),\n",
    "            (\"How do I make sourdough bread?\", \"recipe\"),\n",
    "            (\"Summarize recent transformer papers\", \"scientific\"),\n",
    "            (\"What temperature should I set the equipment/oven to?\", \"clarify\"),\n",
    "            (\"Who won the 2024 Super Bowl?\", \"fallback\"),\n",
    "        ]\n",
    "\n",
    "        results = []\n",
    "        for query_text, expected in queries:\n",
    "            result = run_query(self.workflow, query_text)\n",
    "            timing = result.get(\"timing\", {})\n",
    "            crag_timing = timing.get(\"crag\", {})\n",
    "\n",
    "            checks = {\n",
    "                \"has_response\": bool(result.get(\"response\")),\n",
    "                \"domain_matches\": result.get(\"domain\") == expected,\n",
    "                \"confidence_valid\": 0.0 <= result.get(\"confidence\", -1) <= 1.0,\n",
    "                \"timing_schema_ok\": all(\n",
    "                    k in timing for k in (\"total_s\", \"supervisor_s\", \"agent_s\", \"crag\")\n",
    "                ),\n",
    "                \"crag_schema_ok\": all(\n",
    "                    k in crag_timing\n",
    "                    for k in (\"retrieve_s\", \"grade_s\", \"rewrite_s\", \"generate_s\", \"validate_s\", \"total_s\")\n",
    "                ),\n",
    "            }\n",
    "            passed = all(checks.values())\n",
    "\n",
    "            results.append({\n",
    "                \"query\": query_text,\n",
    "                \"expected\": expected,\n",
    "                \"actual\": result.get(\"domain\"),\n",
    "                \"passed\": passed,\n",
    "                \"checks\": checks,\n",
    "            })\n",
    "            status = \"PASS\" if passed else \"FAIL\"\n",
    "            logger.info(\"Functional %s: %s â†’ %s (expected %s)\", status, query_text[:50], result.get(\"domain\"), expected)\n",
    "\n",
    "        return {\n",
    "            \"pass_rate\": sum(r[\"passed\"] for r in results) / len(results) if results else 0,\n",
    "            \"results\": results,\n",
    "        }\n",
    "\n",
    "    # â”€â”€ Verification: Reliability â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    @staticmethod\n",
    "    def verify_reliability_json_parsing() -> Dict:\n",
    "        \"\"\"Test JSON extraction robustness with known-malformed LLM outputs.\n",
    "\n",
    "        Does NOT require a running model -- tests the static parsing logic.\n",
    "        \"\"\"\n",
    "        import json as _json\n",
    "        from foundation.llm_wrapper import KerasHubChatModel\n",
    "\n",
    "        test_cases = [\n",
    "            ('{\"domain\":\"industrial\",\"confidence\":0.9,\"reasoning\":\"fault code\"}',\n",
    "             {\"domain\": \"industrial\"}),\n",
    "            ('```json\\n{\"domain\":\"recipe\",\"confidence\":0.8,\"reasoning\":\"cooking\"}\\n```',\n",
    "             {\"domain\": \"recipe\"}),\n",
    "            ('Sure! Here is the routing:\\n{\"domain\":\"scientific\",\"confidence\":0.95,\"reasoning\":\"papers\"}\\nHope this helps!',\n",
    "             {\"domain\": \"scientific\"}),\n",
    "            ('user\\nYou are the Supervisor Agent...',\n",
    "             {\"error\": \"parse_failed\"}),\n",
    "            ('[{\"domain\":\"industrial\",\"confidence\":0.9,\"reasoning\":\"only item\"}]',\n",
    "             {\"domain\": \"industrial\"}),\n",
    "            ('{\"confidence\": \"high\", \"domain\": \"recipe\", \"reasoning\": \"baking\"}',\n",
    "             {\"domain\": \"recipe\"}),\n",
    "        ]\n",
    "\n",
    "        results = []\n",
    "        for raw_text, expected_subset in test_cases:\n",
    "            # Directly test the parsing logic (text â†’ dict) without model\n",
    "            text = raw_text.strip()\n",
    "\n",
    "            if \"```\" in text:\n",
    "                parts = text.split(\"```\")\n",
    "                for part in parts[1::2]:\n",
    "                    cleaned = part.strip()\n",
    "                    if cleaned.startswith(\"json\"):\n",
    "                        cleaned = cleaned[4:].strip()\n",
    "                    if cleaned.startswith(\"{\") or cleaned.startswith(\"[\"):\n",
    "                        text = cleaned\n",
    "                        break\n",
    "\n",
    "            text = text.strip()\n",
    "            parsed = None\n",
    "            try:\n",
    "                p = _json.loads(text)\n",
    "                if isinstance(p, list) and len(p) == 1 and isinstance(p[0], dict):\n",
    "                    parsed = p[0]\n",
    "                elif isinstance(p, dict):\n",
    "                    parsed = p\n",
    "            except _json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "            if parsed is None:\n",
    "                start = text.find(\"{\")\n",
    "                end = text.rfind(\"}\")\n",
    "                if start != -1 and end != -1 and end > start:\n",
    "                    try:\n",
    "                        parsed = _json.loads(text[start : end + 1])\n",
    "                    except _json.JSONDecodeError:\n",
    "                        pass\n",
    "\n",
    "            if parsed is None:\n",
    "                parsed = {\"error\": \"parse_failed\"}\n",
    "\n",
    "            match = all(parsed.get(k) == v for k, v in expected_subset.items())\n",
    "            results.append({\n",
    "                \"input\": raw_text[:80],\n",
    "                \"expected_keys\": expected_subset,\n",
    "                \"got\": {k: parsed.get(k) for k in expected_subset},\n",
    "                \"match\": match,\n",
    "            })\n",
    "\n",
    "        passed = sum(r[\"match\"] for r in results)\n",
    "        logger.info(\"Reliability JSON parsing: %d/%d passed\", passed, len(results))\n",
    "        return {\"passed\": passed, \"total\": len(results), \"results\": results}\n",
    "\n",
    "    @staticmethod\n",
    "    def verify_reliability_confidence_clamping() -> Dict:\n",
    "        \"\"\"Test confidence validation edge cases in supervisor.\"\"\"\n",
    "        from orchestration.supervisor import SupervisorAgent\n",
    "\n",
    "        class FakeLLM:\n",
    "            pass\n",
    "\n",
    "        sup = SupervisorAgent.__new__(SupervisorAgent)\n",
    "        sup.threshold = 0.75\n",
    "        sup.valid_domains = {\"industrial\", \"recipe\", \"scientific\", \"clarify\", \"fallback\"}\n",
    "\n",
    "        test_cases = [\n",
    "            ({\"confidence\": 1.5, \"domain\": \"industrial\"}, 1.0),\n",
    "            ({\"confidence\": -0.3, \"domain\": \"recipe\"}, 0.0),\n",
    "            ({\"confidence\": \"bad\", \"domain\": \"scientific\"}, 0.0),\n",
    "            ({\"confidence\": float(\"nan\"), \"domain\": \"industrial\"}, 0.0),\n",
    "            ({\"confidence\": 0.85, \"domain\": \"industrial\"}, 0.85),\n",
    "            ({\"confidence\": 0.85, \"domain\": \"unknown_domain\"}, 0.85),\n",
    "        ]\n",
    "\n",
    "        results = []\n",
    "        for input_dict, expected_conf in test_cases:\n",
    "            routing = sup._build_routing_from_result(input_dict, \"test query\")\n",
    "            conf_ok = abs(routing[\"confidence\"] - expected_conf) < 1e-6\n",
    "            domain_ok = routing[\"domain\"] in sup.valid_domains\n",
    "            results.append({\n",
    "                \"input\": str(input_dict),\n",
    "                \"expected_confidence\": expected_conf,\n",
    "                \"got_confidence\": routing[\"confidence\"],\n",
    "                \"confidence_ok\": conf_ok,\n",
    "                \"domain_valid\": domain_ok,\n",
    "            })\n",
    "\n",
    "        passed = sum(r[\"confidence_ok\"] and r[\"domain_valid\"] for r in results)\n",
    "        logger.info(\"Reliability confidence clamping: %d/%d passed\", passed, len(results))\n",
    "        return {\"passed\": passed, \"total\": len(results), \"results\": results}\n",
    "\n",
    "    # â”€â”€ Verification: Performance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def verify_performance(self, queries: Optional[List] = None) -> Dict:\n",
    "        \"\"\"Measure p50/p95 latency and per-step breakdown.\n",
    "\n",
    "        Run a set of queries and compute latency statistics.\n",
    "        \"\"\"\n",
    "        from orchestration.workflow_graph import run_query\n",
    "\n",
    "        queries = queries or [\n",
    "            \"My S7-1200 PLC is showing fault code F0003\",\n",
    "            \"How do I make a proper roux for gumbo?\",\n",
    "            \"Summarize recent papers on transformer attention\",\n",
    "        ]\n",
    "\n",
    "        timings = []\n",
    "        for q in queries:\n",
    "            result = run_query(self.workflow, q)\n",
    "            t = result.get(\"timing\", {})\n",
    "            timings.append({\n",
    "                \"query\": q[:60],\n",
    "                \"total_s\": t.get(\"total_s\", 0),\n",
    "                \"supervisor_s\": t.get(\"supervisor_s\", 0),\n",
    "                \"agent_s\": t.get(\"agent_s\", 0),\n",
    "                \"crag_retrieve_s\": t.get(\"crag\", {}).get(\"retrieve_s\", 0),\n",
    "                \"crag_grade_s\": t.get(\"crag\", {}).get(\"grade_s\", 0),\n",
    "                \"crag_rewrite_s\": t.get(\"crag\", {}).get(\"rewrite_s\", 0),\n",
    "                \"crag_generate_s\": t.get(\"crag\", {}).get(\"generate_s\", 0),\n",
    "                \"crag_validate_s\": t.get(\"crag\", {}).get(\"validate_s\", 0),\n",
    "            })\n",
    "\n",
    "        totals = sorted(t[\"total_s\"] for t in timings)\n",
    "        n = len(totals)\n",
    "\n",
    "        stats = {\n",
    "            \"n_queries\": n,\n",
    "            \"p50_s\": totals[n // 2] if n else 0,\n",
    "            \"p95_s\": totals[int(n * 0.95)] if n else 0,\n",
    "            \"min_s\": totals[0] if n else 0,\n",
    "            \"max_s\": totals[-1] if n else 0,\n",
    "        }\n",
    "        logger.info(\n",
    "            \"Performance: n=%d  p50=%.1fs  p95=%.1fs  min=%.1fs  max=%.1fs\",\n",
    "            stats[\"n_queries\"], stats[\"p50_s\"], stats[\"p95_s\"],\n",
    "            stats[\"min_s\"], stats[\"max_s\"],\n",
    "        )\n",
    "        return {\"stats\": stats, \"per_query\": timings}\n",
    "\n",
    "    # â”€â”€ Verification: Regression â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def run_full_evaluation(self) -> EvaluationResult:\n",
    "        \"\"\"Run the complete evaluation suite (regression baseline).\"\"\"\n",
    "        logger.info(\"Starting full evaluation...\")\n",
    "\n",
    "        routing_results = self.evaluate_routing()\n",
    "\n",
    "        eval_result = EvaluationResult(\n",
    "            routing_accuracy=routing_results[\"overall_accuracy\"],\n",
    "            routing_by_domain=routing_results[\"per_domain_accuracy\"],\n",
    "            per_query_results=routing_results[\"results\"],\n",
    "            avg_latency_seconds=(\n",
    "                sum(r[\"latency\"] for r in routing_results[\"results\"])\n",
    "                / len(routing_results[\"results\"])\n",
    "                if routing_results[\"results\"] else 0\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        completed = sum(\n",
    "            1 for r in routing_results[\"results\"]\n",
    "            if r.get(\"correct\")\n",
    "        )\n",
    "        eval_result.task_completion_rate = (\n",
    "            completed / len(routing_results[\"results\"])\n",
    "            if routing_results[\"results\"] else 0\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Evaluation complete. Routing accuracy: {eval_result.routing_accuracy:.2%}\"\n",
    "        )\n",
    "        return eval_result\n",
    "''',\n",
    "\n",
    "    'foundation/__init__.py': r'''\n",
    "''',\n",
    "\n",
    "    'foundation/embedding_service.py': r'''\n",
    "\"\"\"\n",
    "Embedding Service\n",
    "=================\n",
    "Wraps sentence-transformers (gte-large-en-v1.5) behind a clean interface.\n",
    "Handles batching, device placement, and provides hooks for domain fine-tuning.\n",
    "\n",
    "Usage:\n",
    "    from foundation.embedding_service import EmbeddingService\n",
    "    embedder = EmbeddingService()\n",
    "    vectors = embedder.embed_documents([\"text 1\", \"text 2\"])\n",
    "    query_vec = embedder.embed_query(\"search this\")\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from config.settings import EmbeddingConfig, CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class EmbeddingService(Embeddings):\n",
    "    \"\"\"\n",
    "    LangChain-compatible embedding service backed by sentence-transformers.\n",
    "    Implements the Embeddings interface so it plugs directly into ChromaDB,\n",
    "    retrievers, and any LangChain component expecting an embedder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[EmbeddingConfig] = None):\n",
    "        self.config = config or CONFIG.embedding\n",
    "        self._model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence-transformers model.\"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "\n",
    "            logger.info(f\"Loading embedding model: {self.config.model_name}\")\n",
    "            self._model = SentenceTransformer(\n",
    "                self.config.model_name,\n",
    "                device=self.config.device,\n",
    "                trust_remote_code=self.config.trust_remote_code,\n",
    "            )\n",
    "            self._model.max_seq_length = self.config.max_seq_length\n",
    "            logger.info(\n",
    "                f\"Embedding model loaded. Dimension: {self.config.dimension}, \"\n",
    "                f\"Max seq length: {self.config.max_seq_length}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load embedding model: {e}\")\n",
    "            raise\n",
    "\n",
    "    # â”€â”€ LangChain Embeddings Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents with batching.\"\"\"\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.config.batch_size):\n",
    "            batch = texts[i : i + self.config.batch_size]\n",
    "            embeddings = self._model.encode(\n",
    "                batch,\n",
    "                show_progress_bar=len(texts) > self.config.batch_size,\n",
    "                normalize_embeddings=True,     # unit vectors for cosine sim\n",
    "                convert_to_numpy=True,\n",
    "            )\n",
    "            all_embeddings.extend(embeddings.tolist())\n",
    "        return all_embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query string.\"\"\"\n",
    "        embedding = self._model.encode(\n",
    "            text,\n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True,\n",
    "        )\n",
    "        return embedding.tolist()\n",
    "\n",
    "    # â”€â”€ Domain Fine-Tuning Hook â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def fine_tune(\n",
    "        self,\n",
    "        train_pairs: List[tuple],\n",
    "        epochs: int = 3,\n",
    "        batch_size: int = 16,\n",
    "        output_path: str = \"./fine_tuned_embeddings\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fine-tune the embedding model on domain-specific (query, positive) pairs\n",
    "        using contrastive learning with hard negatives.\n",
    "\n",
    "        Args:\n",
    "            train_pairs: List of (query, positive_passage) tuples.\n",
    "            epochs: Training epochs.\n",
    "            batch_size: Training batch size.\n",
    "            output_path: Where to save the fine-tuned model.\n",
    "        \"\"\"\n",
    "        from sentence_transformers import InputExample, losses\n",
    "        from torch.utils.data import DataLoader\n",
    "\n",
    "        logger.info(\n",
    "            f\"Fine-tuning embeddings on {len(train_pairs)} pairs \"\n",
    "            f\"for {epochs} epochs\"\n",
    "        )\n",
    "\n",
    "        # Build training examples\n",
    "        examples = [\n",
    "            InputExample(texts=[q, p]) for q, p in train_pairs\n",
    "        ]\n",
    "        loader = DataLoader(examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "        # MultipleNegativesRankingLoss â€” standard for contrastive fine-tuning\n",
    "        loss = losses.MultipleNegativesRankingLoss(self._model)\n",
    "\n",
    "        self._model.fit(\n",
    "            train_objectives=[(loader, loss)],\n",
    "            epochs=epochs,\n",
    "            output_path=output_path,\n",
    "            show_progress_bar=True,\n",
    "        )\n",
    "        logger.info(f\"Fine-tuned model saved to {output_path}\")\n",
    "\n",
    "    def load_fine_tuned(self, path: str):\n",
    "        \"\"\"Load a previously fine-tuned model.\"\"\"\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self._model = SentenceTransformer(path, device=self.config.device)\n",
    "        logger.info(f\"Loaded fine-tuned model from {path}\")\n",
    "''',\n",
    "\n",
    "    'foundation/llm_wrapper.py': r'''\n",
    "\"\"\"\n",
    "KerasHub â†” LangChain Bridge\n",
    "============================\n",
    "Custom wrapper that makes KerasHub models (Gemma 3, Gemma 2, Llama 3.1)\n",
    "compatible with LangChain's BaseChatModel interface, enabling use inside\n",
    "LangGraph agents.\n",
    "\n",
    "This is one of the novel contributions of the project â€” no existing library\n",
    "provides this integration.\n",
    "\n",
    "Usage:\n",
    "    from foundation.llm_wrapper import KerasHubChatModel\n",
    "    llm = KerasHubChatModel(config=CONFIG.llm)\n",
    "    response = llm.invoke([HumanMessage(content=\"Hello\")])\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import Any, Iterator, List, Optional\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from config.settings import LLMConfig, CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# â”€â”€ Prompt Formatters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Each model family has its own control token format.\n",
    "\n",
    "def _format_gemma(messages: List[BaseMessage]) -> str:\n",
    "    \"\"\"Format messages using Gemma 2 control tokens.\"\"\"\n",
    "    parts = []\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, SystemMessage):\n",
    "            # Gemma 2 doesn't have a system role â€” prepend to first user turn\n",
    "            parts.append(f\"<start_of_turn>user\\n[System Instructions]\\n{msg.content}\\n\")\n",
    "        elif isinstance(msg, HumanMessage):\n",
    "            # Avoid double user tag if system was prepended\n",
    "            if parts and parts[-1].startswith(\"<start_of_turn>user\\n[System\"):\n",
    "                parts[-1] += f\"\\n{msg.content}<end_of_turn>\\n\"\n",
    "            else:\n",
    "                parts.append(f\"<start_of_turn>user\\n{msg.content}<end_of_turn>\\n\")\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            parts.append(f\"<start_of_turn>model\\n{msg.content}<end_of_turn>\\n\")\n",
    "    # Open the model turn for generation\n",
    "    parts.append(\"<start_of_turn>model\\n\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "def _format_llama(messages: List[BaseMessage]) -> str:\n",
    "    \"\"\"Format messages using Llama 3.1 control tokens.\"\"\"\n",
    "    parts = [\"<|begin_of_text|>\"]\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, SystemMessage):\n",
    "            parts.append(\n",
    "                f\"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "                f\"{msg.content}<|eot_id|>\"\n",
    "            )\n",
    "        elif isinstance(msg, HumanMessage):\n",
    "            parts.append(\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                f\"{msg.content}<|eot_id|>\"\n",
    "            )\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            parts.append(\n",
    "                f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                f\"{msg.content}<|eot_id|>\"\n",
    "            )\n",
    "    parts.append(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "# Map preset names to formatters\n",
    "_FORMATTERS = {\n",
    "    \"gemma\": _format_gemma,\n",
    "    \"llama\": _format_llama,\n",
    "}\n",
    "\n",
    "def _get_formatter(preset: str):\n",
    "    \"\"\"Select the right prompt formatter based on model preset name.\"\"\"\n",
    "    preset_lower = preset.lower()\n",
    "    for key, fn in _FORMATTERS.items():\n",
    "        if key in preset_lower:\n",
    "            return fn\n",
    "    raise ValueError(\n",
    "        f\"No prompt formatter registered for preset '{preset}'. \"\n",
    "        f\"Supported families: {list(_FORMATTERS.keys())}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# â”€â”€ KerasHub Chat Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class KerasHubChatModel(BaseChatModel):\n",
    "    \"\"\"\n",
    "    LangChain-compatible chat model backed by a KerasHub CausalLM.\n",
    "\n",
    "    Handles:\n",
    "    1. Prompt Construction â€” converts LangChain messages to model-specific\n",
    "       control token format.\n",
    "    2. Inference â€” triggers KerasHub generate() with configured sampling.\n",
    "    3. Output Parsing â€” strips the prompt echo, extracts generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pydantic fields (LangChain requires these to be class-level)\n",
    "    config: LLMConfig = None\n",
    "    model: Any = None           # keras_hub.models.CausalLM (set at init)\n",
    "    _formatter: Any = None\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def __init__(self, config: Optional[LLMConfig] = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config or CONFIG.llm\n",
    "        self._formatter = _get_formatter(self.config.preset)\n",
    "        self._load_model()\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_preset(keras_hub, preset: str, dtype: str):\n",
    "        \"\"\"Load a KerasHub model, using Gemma3CausalLM for gemma3 presets.\"\"\"\n",
    "        if \"gemma3\" in preset.lower():\n",
    "            return keras_hub.models.Gemma3CausalLM.from_preset(preset, dtype=dtype)\n",
    "        return keras_hub.models.CausalLM.from_preset(preset, dtype=dtype)\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the KerasHub model with configured precision.\"\"\"\n",
    "        try:\n",
    "            import os\n",
    "            os.environ[\"KERAS_BACKEND\"] = self.config.backend\n",
    "\n",
    "            import keras_hub\n",
    "\n",
    "            logger.info(f\"Loading KerasHub model: {self.config.preset} \"\n",
    "                        f\"(dtype={self.config.dtype})\")\n",
    "            self.model = self._load_preset(\n",
    "                keras_hub, self.config.preset, self.config.dtype\n",
    "            )\n",
    "            logger.info(\"Model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load primary model: {e}\")\n",
    "            if self.config.fallback_preset:\n",
    "                logger.info(f\"Attempting fallback: {self.config.fallback_preset}\")\n",
    "                self._formatter = _get_formatter(self.config.fallback_preset)\n",
    "                import keras_hub\n",
    "                self.model = self._load_preset(\n",
    "                    keras_hub, self.config.fallback_preset, self.config.dtype\n",
    "                )\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    # â”€â”€ LangChain Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"kerashub-chat\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Core generation method called by LangChain.\"\"\"\n",
    "\n",
    "        # 1. Format prompt\n",
    "        prompt = self._formatter(messages)\n",
    "\n",
    "        # Allow callers to override token budget (e.g. short tasks)\n",
    "        token_budget = kwargs.get(\"max_new_tokens\", self.config.max_new_tokens)\n",
    "\n",
    "        # 2. Run inference (with optional timing for throughput diagnosis)\n",
    "        t0 = time.perf_counter()\n",
    "        raw_output = self.model.generate(\n",
    "            prompt,\n",
    "            max_length=len(prompt) + token_budget,\n",
    "        )\n",
    "        elapsed = time.perf_counter() - t0\n",
    "\n",
    "        # 3. Strip prompt echo â€” KerasHub returns prompt + generation\n",
    "        generated_text = raw_output\n",
    "        if isinstance(raw_output, str) and raw_output.startswith(prompt):\n",
    "            generated_text = raw_output[len(prompt):]\n",
    "        elif isinstance(raw_output, str):\n",
    "            # Fallback for Gemma 3: exact startswith may fail due to\n",
    "            # whitespace/token differences.  Find the last model-turn\n",
    "            # marker and take everything after it.\n",
    "            marker = \"<start_of_turn>model\\n\"\n",
    "            last_model = raw_output.rfind(marker)\n",
    "            if last_model != -1:\n",
    "                generated_text = raw_output[last_model + len(marker):]\n",
    "\n",
    "        # 4. Clean up trailing control tokens\n",
    "        for token in [\"<end_of_turn>\", \"<|eot_id|>\", \"<|end_of_text|>\"]:\n",
    "            generated_text = generated_text.split(token)[0]\n",
    "\n",
    "        generated_text = generated_text.strip()\n",
    "\n",
    "        # 5. Log throughput (approx: ~4 chars/token for English)\n",
    "        est_tokens = max(1, len(generated_text) // 4)\n",
    "        tps = est_tokens / elapsed if elapsed > 0 else 0\n",
    "        logger.info(\n",
    "            f\"LLM call: {elapsed:.1f}s | ~{est_tokens} output tokens | \"\n",
    "            f\"~{tps:.1f} tok/s\"\n",
    "        )\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=[\n",
    "                ChatGeneration(message=AIMessage(content=generated_text))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # â”€â”€ Convenience â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def invoke_and_parse_json(\n",
    "        self, messages: List[BaseMessage], **kwargs\n",
    "    ) -> dict:\n",
    "        \"\"\"Generate and parse a JSON response. Useful for grader/router.\n",
    "\n",
    "        Uses the short token budget by default since structured JSON\n",
    "        outputs (routing, grading, validation) are compact.\n",
    "        \"\"\"\n",
    "        kwargs.setdefault(\"max_new_tokens\", self.config.short_max_new_tokens)\n",
    "        result = self.invoke(messages, **kwargs)\n",
    "        text = result.content.strip()\n",
    "\n",
    "        # Strip markdown code fences (handles ```json ... ``` and ``` ... ```)\n",
    "        if \"```\" in text:\n",
    "            parts = text.split(\"```\")\n",
    "            for part in parts[1::2]:  # odd-indexed parts are inside fences\n",
    "                cleaned = part.strip()\n",
    "                if cleaned.startswith(\"json\"):\n",
    "                    cleaned = cleaned[4:].strip()\n",
    "                if cleaned.startswith(\"{\") or cleaned.startswith(\"[\"):\n",
    "                    text = cleaned\n",
    "                    break\n",
    "\n",
    "        text = text.strip()\n",
    "\n",
    "        # Try direct parse\n",
    "        try:\n",
    "            parsed = json.loads(text)\n",
    "            if isinstance(parsed, list) and len(parsed) == 1 and isinstance(parsed[0], dict):\n",
    "                return parsed[0]\n",
    "            if isinstance(parsed, dict):\n",
    "                return parsed\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Fallback: extract first { ... } substring\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            try:\n",
    "                return json.loads(text[start : end + 1])\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "        logger.warning(\"JSON parse failed, raw output: %s\", text[:200])\n",
    "        return {\"error\": \"parse_failed\", \"raw\": text}\n",
    "''',\n",
    "\n",
    "    'foundation/vector_store.py': r'''\n",
    "\"\"\"\n",
    "Vector Store Service\n",
    "====================\n",
    "ChromaDB abstraction with multi-domain collection management.\n",
    "Each domain (industrial, recipe, scientific) gets its own isolated collection.\n",
    "\n",
    "Usage:\n",
    "    from foundation.vector_store import VectorStoreService\n",
    "    vs = VectorStoreService(embedding_service=embedder)\n",
    "    vs.add_documents(\"industrial\", docs)\n",
    "    results = vs.search(\"industrial\", \"PLC fault code F0003\", k=5)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings as ChromaSettings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from foundation.embedding_service import EmbeddingService\n",
    "from config.settings import VectorStoreConfig, CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class VectorStoreService:\n",
    "    \"\"\"\n",
    "    Multi-domain vector store backed by ChromaDB.\n",
    "\n",
    "    Key features:\n",
    "    - One collection per domain for isolated retrieval\n",
    "    - Consistent interface for add / search / delete\n",
    "    - LangChain Document in/out for pipeline compatibility\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_service: EmbeddingService,\n",
    "        config: Optional[VectorStoreConfig] = None,\n",
    "    ):\n",
    "        self.config = config or CONFIG.vector_store\n",
    "        self.embedder = embedding_service\n",
    "        self._client = None\n",
    "        self._collections: Dict[str, chromadb.Collection] = {}\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        \"\"\"Initialize ChromaDB client and create collections.\"\"\"\n",
    "        self._client = chromadb.Client(\n",
    "            ChromaSettings(\n",
    "                persist_directory=self.config.persist_directory,\n",
    "                anonymized_telemetry=False,\n",
    "            )\n",
    "        )\n",
    "        # Create or get each domain collection\n",
    "        for domain, collection_name in self.config.collections.items():\n",
    "            self._collections[domain] = self._client.get_or_create_collection(\n",
    "                name=collection_name,\n",
    "                metadata={\"hnsw:space\": self.config.similarity_metric},\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Collection '{collection_name}' ready \"\n",
    "                f\"(count: {self._collections[domain].count()})\"\n",
    "            )\n",
    "\n",
    "    # â”€â”€ Core Operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def add_documents(\n",
    "        self,\n",
    "        domain: str,\n",
    "        documents: List[Document],\n",
    "        batch_size: int = 100,\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Add LangChain Documents to a domain-specific collection.\n",
    "\n",
    "        Returns the number of documents added.\n",
    "        \"\"\"\n",
    "        self._validate_domain(domain)\n",
    "        collection = self._collections[domain]\n",
    "\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        metadatas = [doc.metadata for doc in documents]\n",
    "        ids = [\n",
    "            doc.metadata.get(\"id\", f\"{domain}_{i}\")\n",
    "            for i, doc in enumerate(documents)\n",
    "        ]\n",
    "\n",
    "        # Embed and upsert in batches\n",
    "        added = 0\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "            batch_meta = metadatas[i : i + batch_size]\n",
    "            batch_ids = ids[i : i + batch_size]\n",
    "            batch_embeddings = self.embedder.embed_documents(batch_texts)\n",
    "\n",
    "            collection.upsert(\n",
    "                ids=batch_ids,\n",
    "                embeddings=batch_embeddings,\n",
    "                documents=batch_texts,\n",
    "                metadatas=batch_meta,\n",
    "            )\n",
    "            added += len(batch_texts)\n",
    "\n",
    "        logger.info(f\"Added {added} documents to '{domain}' collection\")\n",
    "        return added\n",
    "\n",
    "    def _embed_query_cached(self, query: str) -> Tuple[float, ...]:\n",
    "        \"\"\"Cache query embeddings to avoid re-encoding on CRAG retries.\"\"\"\n",
    "        return self.__embed_cache(query)\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    def __embed_cache(self, query: str) -> Tuple[float, ...]:\n",
    "        return tuple(self.embedder.embed_query(query))\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        domain: str,\n",
    "        query: str,\n",
    "        k: Optional[int] = None,\n",
    "        where: Optional[dict] = None,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Semantic search within a domain collection.\n",
    "\n",
    "        Returns LangChain Documents with metadata including similarity score.\n",
    "        \"\"\"\n",
    "        self._validate_domain(domain)\n",
    "        k = k or self.config.search_top_k\n",
    "        collection = self._collections[domain]\n",
    "\n",
    "        query_embedding = list(self._embed_query_cached(query))\n",
    "\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=k,\n",
    "            where=where,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "        )\n",
    "\n",
    "        documents = []\n",
    "        for doc_text, metadata, distance in zip(\n",
    "            results[\"documents\"][0],\n",
    "            results[\"metadatas\"][0],\n",
    "            results[\"distances\"][0],\n",
    "        ):\n",
    "            metadata[\"similarity_score\"] = 1 - distance  # cosine distance â†’ sim\n",
    "            documents.append(\n",
    "                Document(page_content=doc_text, metadata=metadata)\n",
    "            )\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def get_collection_stats(self, domain: str) -> dict:\n",
    "        \"\"\"Get stats for a domain collection.\"\"\"\n",
    "        self._validate_domain(domain)\n",
    "        collection = self._collections[domain]\n",
    "        return {\n",
    "            \"domain\": domain,\n",
    "            \"collection_name\": collection.name,\n",
    "            \"document_count\": collection.count(),\n",
    "        }\n",
    "\n",
    "    def get_all_stats(self) -> List[dict]:\n",
    "        \"\"\"Get stats for all domain collections.\"\"\"\n",
    "        return [self.get_collection_stats(d) for d in self._collections]\n",
    "\n",
    "    def clear_collection(self, domain: str):\n",
    "        \"\"\"Delete all documents in a domain collection.\"\"\"\n",
    "        self._validate_domain(domain)\n",
    "        collection_name = self.config.collections[domain]\n",
    "        self._client.delete_collection(collection_name)\n",
    "        self._collections[domain] = self._client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": self.config.similarity_metric},\n",
    "        )\n",
    "        logger.info(f\"Cleared collection for domain '{domain}'\")\n",
    "\n",
    "    # â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def _validate_domain(self, domain: str):\n",
    "        if domain not in self._collections:\n",
    "            raise ValueError(\n",
    "                f\"Unknown domain '{domain}'. \"\n",
    "                f\"Available: {list(self._collections.keys())}\"\n",
    "            )\n",
    "''',\n",
    "\n",
    "    'ingestion/__init__.py': r'''\n",
    "''',\n",
    "\n",
    "    'ingestion/chunking_pipeline.py': r'''\n",
    "\"\"\"\n",
    "Chunking Pipeline\n",
    "=================\n",
    "Splits documents into retrieval-optimized chunks with configurable overlap.\n",
    "Industrial documents get extra preprocessing (abbreviation expansion,\n",
    "equipment code normalization).\n",
    "\n",
    "Usage:\n",
    "    from ingestion.chunking_pipeline import ChunkingPipeline\n",
    "    chunker = ChunkingPipeline()\n",
    "    chunks = chunker.chunk_documents(docs, domain=\"industrial\")\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import re\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from config.settings import ChunkingConfig, CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# â”€â”€ Industrial Domain Preprocessors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Common industrial abbreviations â†’ expanded forms\n",
    "INDUSTRIAL_ABBREVIATIONS: Dict[str, str] = {\n",
    "    \"PLC\": \"Programmable Logic Controller (PLC)\",\n",
    "    \"HMI\": \"Human-Machine Interface (HMI)\",\n",
    "    \"SCADA\": \"Supervisory Control and Data Acquisition (SCADA)\",\n",
    "    \"VFD\": \"Variable Frequency Drive (VFD)\",\n",
    "    \"RTU\": \"Remote Terminal Unit (RTU)\",\n",
    "    \"DCS\": \"Distributed Control System (DCS)\",\n",
    "    \"I/O\": \"Input/Output (I/O)\",\n",
    "    \"OPC\": \"Open Platform Communications (OPC)\",\n",
    "    \"SIL\": \"Safety Integrity Level (SIL)\",\n",
    "    \"MTBF\": \"Mean Time Between Failures (MTBF)\",\n",
    "    \"MTTR\": \"Mean Time To Repair (MTTR)\",\n",
    "    \"PM\": \"Preventive Maintenance (PM)\",\n",
    "    \"CM\": \"Corrective Maintenance (CM)\",\n",
    "    \"NER\": \"Named Entity Recognition (NER)\",\n",
    "}\n",
    "\n",
    "# Equipment code patterns (e.g., S7-1200, 6ES7-214, AB-1756)\n",
    "EQUIPMENT_CODE_PATTERN = re.compile(\n",
    "    r'\\b([A-Z]{1,4}[-/]?\\d{1,5}[-/]?\\d{0,5}[A-Z]{0,3})\\b'\n",
    ")\n",
    "\n",
    "\n",
    "def _preprocess_industrial(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Industrial-specific text preprocessing:\n",
    "    1. Expand abbreviations (first occurrence only to avoid bloat)\n",
    "    2. Normalize equipment codes for consistent retrieval\n",
    "    3. Normalize whitespace\n",
    "    \"\"\"\n",
    "    # Expand abbreviations â€” only first occurrence per abbreviation\n",
    "    expanded = set()\n",
    "    for abbrev, full in INDUSTRIAL_ABBREVIATIONS.items():\n",
    "        if abbrev in text and abbrev not in expanded:\n",
    "            # Replace first standalone occurrence\n",
    "            text = re.sub(\n",
    "                rf'\\b{re.escape(abbrev)}\\b',\n",
    "                full,\n",
    "                text,\n",
    "                count=1,\n",
    "            )\n",
    "            expanded.add(abbrev)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Domain preprocessor registry\n",
    "_PREPROCESSORS = {\n",
    "    \"industrial\": _preprocess_industrial,\n",
    "}\n",
    "\n",
    "\n",
    "# â”€â”€ Chunking Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class ChunkingPipeline:\n",
    "    \"\"\"\n",
    "    Document chunking with domain-aware preprocessing.\n",
    "\n",
    "    Uses RecursiveCharacterTextSplitter under the hood with configurable\n",
    "    chunk size, overlap, and separators. Industrial documents are preprocessed\n",
    "    before chunking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[ChunkingConfig] = None):\n",
    "        self.config = config or CONFIG.chunking\n",
    "        # Approximate tokens â†’ chars (1 token â‰ˆ 4 chars)\n",
    "        self._char_chunk_size = self.config.chunk_size * 4\n",
    "        self._char_overlap = int(self._char_chunk_size * self.config.chunk_overlap_pct)\n",
    "\n",
    "        self._splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self._char_chunk_size,\n",
    "            chunk_overlap=self._char_overlap,\n",
    "            separators=self.config.separators,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "\n",
    "    def chunk_documents(\n",
    "        self,\n",
    "        documents: List[Document],\n",
    "        domain: str = \"general\",\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Chunk a list of documents, applying domain-specific preprocessing.\n",
    "\n",
    "        Each output chunk inherits the parent document's metadata plus:\n",
    "        - chunk_index: position within the parent\n",
    "        - parent_id: ID of the source document\n",
    "        \"\"\"\n",
    "        preprocessor = _PREPROCESSORS.get(domain)\n",
    "        all_chunks = []\n",
    "\n",
    "        for doc in documents:\n",
    "            # Apply domain preprocessing if available\n",
    "            text = doc.page_content\n",
    "            if preprocessor:\n",
    "                text = preprocessor(text)\n",
    "\n",
    "            # Split into chunks\n",
    "            text_chunks = self._splitter.split_text(text)\n",
    "\n",
    "            parent_id = doc.metadata.get(\"id\", doc.metadata.get(\"source\", \"unknown\"))\n",
    "\n",
    "            for i, chunk_text in enumerate(text_chunks):\n",
    "                chunk_metadata = {\n",
    "                    **doc.metadata,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(text_chunks),\n",
    "                    \"parent_id\": parent_id,\n",
    "                    \"id\": f\"{parent_id}_chunk_{i}\",\n",
    "                    \"domain\": domain,\n",
    "                }\n",
    "                all_chunks.append(\n",
    "                    Document(page_content=chunk_text, metadata=chunk_metadata)\n",
    "                )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Chunked {len(documents)} documents into {len(all_chunks)} chunks \"\n",
    "            f\"(domain={domain})\"\n",
    "        )\n",
    "        return all_chunks\n",
    "\n",
    "    def chunk_single(self, text: str, domain: str = \"general\") -> List[str]:\n",
    "        \"\"\"Chunk a single text string. Returns raw strings.\"\"\"\n",
    "        preprocessor = _PREPROCESSORS.get(domain)\n",
    "        if preprocessor:\n",
    "            text = preprocessor(text)\n",
    "        return self._splitter.split_text(text)\n",
    "''',\n",
    "\n",
    "    'ingestion/document_loader.py': r'''\n",
    "\"\"\"\n",
    "Document Loader\n",
    "===============\n",
    "Unified loader that normalizes different source formats into LangChain Documents.\n",
    "Supports: PDF, text/markdown, CSV (Food.com), ArXiv API, and raw strings.\n",
    "\n",
    "Usage:\n",
    "    from ingestion.document_loader import DocumentLoader\n",
    "    loader = DocumentLoader()\n",
    "    docs = loader.load_pdf(\"manual.pdf\")\n",
    "    docs = loader.load_food_csv(\"RAW_recipes.csv\", max_rows=10000)\n",
    "    docs = loader.load_arxiv(\"transformer attention mechanisms\", max_results=5)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DocumentLoader:\n",
    "    \"\"\"Unified document loader producing standardized LangChain Documents.\"\"\"\n",
    "\n",
    "    # â”€â”€ PDF / Text Files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pdf(path: str, source_label: Optional[str] = None) -> List[Document]:\n",
    "        \"\"\"Load a PDF file using PyPDF2.\"\"\"\n",
    "        from PyPDF2 import PdfReader\n",
    "\n",
    "        reader = PdfReader(path)\n",
    "        docs = []\n",
    "        label = source_label or Path(path).stem\n",
    "\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            text = page.extract_text()\n",
    "            if text and text.strip():\n",
    "                docs.append(Document(\n",
    "                    page_content=text.strip(),\n",
    "                    metadata={\n",
    "                        \"source\": label,\n",
    "                        \"file_path\": path,\n",
    "                        \"page\": i + 1,\n",
    "                        \"type\": \"pdf\",\n",
    "                    },\n",
    "                ))\n",
    "        logger.info(f\"Loaded {len(docs)} pages from PDF: {path}\")\n",
    "        return docs\n",
    "\n",
    "    @staticmethod\n",
    "    def load_text(path: str, source_label: Optional[str] = None) -> List[Document]:\n",
    "        \"\"\"Load a plain text or markdown file.\"\"\"\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        label = source_label or Path(path).stem\n",
    "        return [Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"source\": label,\n",
    "                \"file_path\": path,\n",
    "                \"type\": \"text\",\n",
    "            },\n",
    "        )]\n",
    "\n",
    "    @staticmethod\n",
    "    def load_directory(\n",
    "        dir_path: str,\n",
    "        extensions: tuple = (\".pdf\", \".txt\", \".md\"),\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Load all supported files from a directory.\"\"\"\n",
    "        docs = []\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for fname in sorted(files):\n",
    "                fpath = os.path.join(root, fname)\n",
    "                ext = Path(fname).suffix.lower()\n",
    "                if ext == \".pdf\":\n",
    "                    docs.extend(DocumentLoader.load_pdf(fpath))\n",
    "                elif ext in (\".txt\", \".md\"):\n",
    "                    docs.extend(DocumentLoader.load_text(fpath))\n",
    "        logger.info(f\"Loaded {len(docs)} documents from directory: {dir_path}\")\n",
    "        return docs\n",
    "\n",
    "    # â”€â”€ Food.com CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    @staticmethod\n",
    "    def load_food_csv(\n",
    "        path: str,\n",
    "        max_rows: Optional[int] = None,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load Food.com RAW_recipes.csv into Documents.\n",
    "        Each row becomes one Document with structured recipe content.\n",
    "        \"\"\"\n",
    "        docs = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for i, row in enumerate(reader):\n",
    "                if max_rows and i >= max_rows:\n",
    "                    break\n",
    "\n",
    "                # Build readable recipe text\n",
    "                content = _format_recipe(row)\n",
    "                if not content.strip():\n",
    "                    continue\n",
    "\n",
    "                docs.append(Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": \"food.com\",\n",
    "                        \"recipe_id\": row.get(\"id\", str(i)),\n",
    "                        \"name\": row.get(\"name\", \"\"),\n",
    "                        \"minutes\": row.get(\"minutes\", \"\"),\n",
    "                        \"n_ingredients\": row.get(\"n_ingredients\", \"\"),\n",
    "                        \"type\": \"recipe\",\n",
    "                        \"id\": f\"recipe_{row.get('id', i)}\",\n",
    "                    },\n",
    "                ))\n",
    "\n",
    "        logger.info(f\"Loaded {len(docs)} recipes from: {path}\")\n",
    "        return docs\n",
    "\n",
    "    # â”€â”€ ArXiv API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    @staticmethod\n",
    "    def load_arxiv(\n",
    "        query: str,\n",
    "        max_results: int = 5,\n",
    "        sort_by: str = \"relevance\",\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Fetch papers from ArXiv API.\n",
    "        Returns Documents with abstract + metadata (no full text by default).\n",
    "        \"\"\"\n",
    "        import arxiv\n",
    "\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=(\n",
    "                arxiv.SortCriterion.Relevance\n",
    "                if sort_by == \"relevance\"\n",
    "                else arxiv.SortCriterion.SubmittedDate\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        docs = []\n",
    "        for paper in search.results():\n",
    "            content = (\n",
    "                f\"Title: {paper.title}\\n\\n\"\n",
    "                f\"Authors: {', '.join(a.name for a in paper.authors)}\\n\\n\"\n",
    "                f\"Abstract: {paper.summary}\\n\\n\"\n",
    "                f\"Published: {paper.published.strftime('%Y-%m-%d')}\\n\"\n",
    "                f\"ArXiv ID: {paper.entry_id}\"\n",
    "            )\n",
    "            docs.append(Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"source\": \"arxiv\",\n",
    "                    \"arxiv_id\": paper.entry_id,\n",
    "                    \"title\": paper.title,\n",
    "                    \"authors\": [a.name for a in paper.authors],\n",
    "                    \"published\": paper.published.isoformat(),\n",
    "                    \"pdf_url\": paper.pdf_url,\n",
    "                    \"type\": \"scientific\",\n",
    "                    \"id\": f\"arxiv_{paper.entry_id.split('/')[-1]}\",\n",
    "                },\n",
    "            ))\n",
    "\n",
    "        logger.info(f\"Fetched {len(docs)} papers from ArXiv for: '{query}'\")\n",
    "        return docs\n",
    "\n",
    "    # â”€â”€ Raw Text Helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    @staticmethod\n",
    "    def from_texts(\n",
    "        texts: List[str],\n",
    "        domain: str,\n",
    "        source: str = \"manual\",\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Create Documents from raw text strings.\"\"\"\n",
    "        return [\n",
    "            Document(\n",
    "                page_content=t,\n",
    "                metadata={\n",
    "                    \"source\": source,\n",
    "                    \"type\": domain,\n",
    "                    \"id\": f\"{domain}_{i}\",\n",
    "                },\n",
    "            )\n",
    "            for i, t in enumerate(texts)\n",
    "        ]\n",
    "\n",
    "\n",
    "# â”€â”€ Private Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def _format_recipe(row: dict) -> str:\n",
    "    \"\"\"Format a Food.com CSV row into readable recipe text.\"\"\"\n",
    "    parts = []\n",
    "    name = row.get(\"name\", \"Untitled Recipe\")\n",
    "    parts.append(f\"Recipe: {name}\")\n",
    "\n",
    "    if row.get(\"minutes\"):\n",
    "        parts.append(f\"Prep Time: {row['minutes']} minutes\")\n",
    "\n",
    "    if row.get(\"description\"):\n",
    "        parts.append(f\"Description: {row['description']}\")\n",
    "\n",
    "    # Ingredients â€” stored as Python list string in CSV\n",
    "    ingredients = row.get(\"ingredients\", \"\")\n",
    "    if ingredients:\n",
    "        try:\n",
    "            import ast\n",
    "            ing_list = ast.literal_eval(ingredients)\n",
    "            parts.append(\"Ingredients: \" + \", \".join(ing_list))\n",
    "        except (ValueError, SyntaxError):\n",
    "            parts.append(f\"Ingredients: {ingredients}\")\n",
    "\n",
    "    # Steps\n",
    "    steps = row.get(\"steps\", \"\")\n",
    "    if steps:\n",
    "        try:\n",
    "            import ast\n",
    "            step_list = ast.literal_eval(steps)\n",
    "            parts.append(\"Steps:\\n\" + \"\\n\".join(\n",
    "                f\"  {i+1}. {s}\" for i, s in enumerate(step_list)\n",
    "            ))\n",
    "        except (ValueError, SyntaxError):\n",
    "            parts.append(f\"Steps: {steps}\")\n",
    "\n",
    "    if row.get(\"nutrition\"):\n",
    "        parts.append(f\"Nutrition: {row['nutrition']}\")\n",
    "\n",
    "    return \"\\n\".join(parts)\n",
    "''',\n",
    "\n",
    "    'ingestion/index_builder.py': r'''\n",
    "\"\"\"\n",
    "Index Builder\n",
    "=============\n",
    "Orchestrates the full ingestion pipeline: load â†’ chunk â†’ embed â†’ store.\n",
    "One-time setup per domain. Provides convenience methods for each domain type.\n",
    "\n",
    "Usage:\n",
    "    from ingestion.index_builder import IndexBuilder\n",
    "    builder = IndexBuilder(vector_store=vs)\n",
    "\n",
    "    # Industrial domain\n",
    "    builder.index_industrial_docs(\"./data/manuals/\")\n",
    "\n",
    "    # Recipe domain\n",
    "    builder.index_recipes(\"./data/RAW_recipes.csv\", max_rows=50000)\n",
    "\n",
    "    # Scientific domain (on-demand, not pre-indexed)\n",
    "    builder.index_arxiv_papers(\"transformer attention\", max_results=10)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from ingestion.document_loader import DocumentLoader\n",
    "from ingestion.chunking_pipeline import ChunkingPipeline\n",
    "from foundation.vector_store import VectorStoreService\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class IndexBuilder:\n",
    "    \"\"\"\n",
    "    End-to-end indexing orchestrator.\n",
    "    Connects document loading, chunking, and vector storage.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: VectorStoreService,\n",
    "        chunker: Optional[ChunkingPipeline] = None,\n",
    "    ):\n",
    "        self.vector_store = vector_store\n",
    "        self.chunker = chunker or ChunkingPipeline()\n",
    "        self.loader = DocumentLoader()\n",
    "\n",
    "    # â”€â”€ Industrial Domain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def index_industrial_docs(self, directory: str) -> int:\n",
    "        \"\"\"\n",
    "        Index all PDFs and text files from an industrial manuals directory.\n",
    "        Applies industrial-specific preprocessing during chunking.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Indexing industrial documents from: {directory}\")\n",
    "        docs = self.loader.load_directory(directory)\n",
    "        chunks = self.chunker.chunk_documents(docs, domain=\"industrial\")\n",
    "        count = self.vector_store.add_documents(\"industrial\", chunks)\n",
    "        logger.info(f\"Industrial indexing complete: {count} chunks stored\")\n",
    "        return count\n",
    "\n",
    "    def index_industrial_texts(self, texts: List[str], source: str = \"manual\") -> int:\n",
    "        \"\"\"Index raw text strings as industrial documents.\"\"\"\n",
    "        docs = self.loader.from_texts(texts, domain=\"industrial\", source=source)\n",
    "        chunks = self.chunker.chunk_documents(docs, domain=\"industrial\")\n",
    "        return self.vector_store.add_documents(\"industrial\", chunks)\n",
    "\n",
    "    # â”€â”€ Recipe Domain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def index_recipes(self, csv_path: str, max_rows: Optional[int] = None) -> int:\n",
    "        \"\"\"\n",
    "        Index Food.com recipes from CSV.\n",
    "        Recipes are already semi-structured so chunking is lighter.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Indexing recipes from: {csv_path}\")\n",
    "        docs = self.loader.load_food_csv(csv_path, max_rows=max_rows)\n",
    "        # Recipes are typically short enough to be single chunks,\n",
    "        # but we chunk anyway for consistency\n",
    "        chunks = self.chunker.chunk_documents(docs, domain=\"recipe\")\n",
    "        count = self.vector_store.add_documents(\"recipe\", chunks)\n",
    "        logger.info(f\"Recipe indexing complete: {count} chunks stored\")\n",
    "        return count\n",
    "\n",
    "    # â”€â”€ Scientific Domain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def index_arxiv_papers(\n",
    "        self,\n",
    "        query: str,\n",
    "        max_results: int = 10,\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Fetch and index ArXiv papers for a given query.\n",
    "        This is typically called on-demand rather than as a one-time setup.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Fetching ArXiv papers for: '{query}'\")\n",
    "        docs = self.loader.load_arxiv(query, max_results=max_results)\n",
    "        chunks = self.chunker.chunk_documents(docs, domain=\"scientific\")\n",
    "        count = self.vector_store.add_documents(\"scientific\", chunks)\n",
    "        logger.info(f\"Scientific indexing complete: {count} chunks stored\")\n",
    "        return count\n",
    "\n",
    "    # â”€â”€ Generic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    def index_documents(\n",
    "        self,\n",
    "        documents: List[Document],\n",
    "        domain: str,\n",
    "    ) -> int:\n",
    "        \"\"\"Index pre-loaded documents into a specific domain.\"\"\"\n",
    "        chunks = self.chunker.chunk_documents(documents, domain=domain)\n",
    "        return self.vector_store.add_documents(domain, chunks)\n",
    "\n",
    "    def get_status(self) -> dict:\n",
    "        \"\"\"Return indexing status across all domains.\"\"\"\n",
    "        return {\n",
    "            \"collections\": self.vector_store.get_all_stats(),\n",
    "        }\n",
    "''',\n",
    "\n",
    "    'orchestration/__init__.py': r'''\n",
    "''',\n",
    "\n",
    "    'orchestration/state_schema.py': r'''\n",
    "\"\"\"\n",
    "State Schema\n",
    "=============\n",
    "Defines the shared state that flows through the LangGraph workflow.\n",
    "All agents read from and write to this state object.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Literal, Optional, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict, total=False):\n",
    "    \"\"\"\n",
    "    Shared state for the LangGraph supervisor workflow.\n",
    "\n",
    "    This TypedDict defines every field that can flow through the graph.\n",
    "    Nodes read what they need and write their outputs back.\n",
    "    \"\"\"\n",
    "\n",
    "    # â”€â”€ User Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    user_query: str                          # Original user query\n",
    "    conversation_history: List[BaseMessage]  # Full conversation context\n",
    "\n",
    "    # â”€â”€ Routing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    routed_domain: str                       # \"industrial\" | \"recipe\" | \"scientific\" | \"clarify\" | \"fallback\"\n",
    "    routing_confidence: float                # 0.0 - 1.0\n",
    "    routing_reasoning: str                   # Why this domain was chosen\n",
    "\n",
    "    # â”€â”€ RAG State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    retrieved_documents: List[Dict]          # Raw retrieved docs\n",
    "    relevant_documents: List[Dict]           # After grading\n",
    "    current_query: str                       # May differ from user_query after rewriting\n",
    "    rewrite_count: int                       # Number of query rewrites so far\n",
    "\n",
    "    # â”€â”€ Agent Output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    agent_response: str                      # The generated response text\n",
    "    agent_sources: List[Dict]                # Source attribution\n",
    "    agent_confidence: float                  # Response confidence score\n",
    "    agent_grounded: bool                     # Hallucination check result\n",
    "\n",
    "    # â”€â”€ Control Flow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    escalated: bool                          # Whether agent escalated back\n",
    "    escalation_reason: str                   # Why escalation happened\n",
    "    needs_clarification: bool                # Supervisor requests more info\n",
    "    clarification_message: str               # What to ask the user\n",
    "    final_response: str                      # The response to return to user\n",
    "    status: str                              # \"routing\" | \"processing\" | \"complete\" | \"escalated\"\n",
    "\n",
    "    # â”€â”€ Timing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    timing_supervisor_s: float               # Supervisor routing time\n",
    "    timing_agent_s: float                    # Agent (CRAG) processing time\n",
    "    timing_clarify_s: float                  # Clarification generation time\n",
    "    timing_crag_breakdown: Dict              # Per-step CRAG timing\n",
    "\n",
    "\n",
    "def create_initial_state(query: str, history: Optional[List[BaseMessage]] = None) -> AgentState:\n",
    "    \"\"\"Create a fresh state for a new query.\"\"\"\n",
    "    return AgentState(\n",
    "        user_query=query,\n",
    "        conversation_history=history or [],\n",
    "        routed_domain=\"\",\n",
    "        routing_confidence=0.0,\n",
    "        routing_reasoning=\"\",\n",
    "        retrieved_documents=[],\n",
    "        relevant_documents=[],\n",
    "        current_query=query,\n",
    "        rewrite_count=0,\n",
    "        agent_response=\"\",\n",
    "        agent_sources=[],\n",
    "        agent_confidence=0.0,\n",
    "        agent_grounded=True,\n",
    "        escalated=False,\n",
    "        escalation_reason=\"\",\n",
    "        needs_clarification=False,\n",
    "        clarification_message=\"\",\n",
    "        final_response=\"\",\n",
    "        status=\"routing\",\n",
    "    )\n",
    "''',\n",
    "\n",
    "    'orchestration/supervisor.py': r'''\n",
    "\"\"\"\n",
    "Supervisor Agent\n",
    "================\n",
    "Central router that classifies user intent and delegates to specialists.\n",
    "Implements confidence-based routing with clarification fallback.\n",
    "\n",
    "Usage:\n",
    "    from orchestration.supervisor import SupervisorAgent\n",
    "    supervisor = SupervisorAgent(llm=llm)\n",
    "    routing = supervisor.route(\"My PLC is showing fault code F0003\")\n",
    "    # â†’ {\"domain\": \"industrial\", \"confidence\": 0.98, ...}\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import Dict\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from config.prompts import SUPERVISOR_SYSTEM_PROMPT\n",
    "from config.settings import CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SupervisorAgent:\n",
    "    \"\"\"\n",
    "    Supervisor/Router Agent.\n",
    "\n",
    "    Analyzes user queries and routes to the appropriate specialist agent.\n",
    "    Uses few-shot prompting and confidence scoring.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.threshold = CONFIG.supervisor.routing_confidence_threshold\n",
    "        self.valid_domains = set(CONFIG.supervisor.domains + [\"clarify\", \"fallback\"])\n",
    "\n",
    "    def route(self, query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Classify and route a user query.\n",
    "\n",
    "        Returns:\n",
    "            {\n",
    "                \"domain\": str,         # target domain or \"clarify\"/\"fallback\"\n",
    "                \"confidence\": float,   # 0.0 - 1.0\n",
    "                \"reasoning\": str,      # explanation\n",
    "            }\n",
    "        \"\"\"\n",
    "        system_msg = SystemMessage(content=SUPERVISOR_SYSTEM_PROMPT)\n",
    "        user_prompt = (\n",
    "            f'User query: \"{query}\"\\n\\n'\n",
    "            \"Output the routing as a single JSON object with keys \"\n",
    "            '\"domain\", \"confidence\", and \"reasoning\". '\n",
    "            \"Respond with ONLY that JSON object, with no extra text.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = self.llm.invoke_and_parse_json(\n",
    "                [system_msg, HumanMessage(content=user_prompt)]\n",
    "            )\n",
    "\n",
    "            if \"error\" in result:\n",
    "                logger.warning(\n",
    "                    \"Supervisor parse failed (rich prompt), raw: %s\",\n",
    "                    result.get(\"raw\", \"\")[:120],\n",
    "                )\n",
    "                # Second attempt: fall back to a simpler, JSON-only prompt.\n",
    "                backup = self._route_with_simple_prompt(query)\n",
    "                if backup is not None:\n",
    "                    return backup\n",
    "                return self._fallback_routing(query)\n",
    "\n",
    "            return self._build_routing_from_result(result, query)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Supervisor routing failed: {e}\")\n",
    "            backup = self._route_with_simple_prompt(query)\n",
    "            if backup is not None:\n",
    "                return backup\n",
    "            return self._fallback_routing(query)\n",
    "\n",
    "    def generate_clarification(self, query: str, routing: Dict) -> str:\n",
    "        \"\"\"Generate a clarification question when routing is ambiguous.\"\"\"\n",
    "        prompt = (\n",
    "            f\"The user asked: \\\"{query}\\\"\\n\\n\"\n",
    "            f\"This query is ambiguous â€” it could relate to multiple domains. \"\n",
    "            f\"Routing analysis: {routing.get('reasoning', 'unclear intent')}\\n\\n\"\n",
    "            f\"Generate a SHORT, friendly clarification question to determine \"\n",
    "            f\"the user's intent. Ask about the specific domain they need help with.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            return result.content.strip()\n",
    "        except Exception:\n",
    "            return (\n",
    "                \"I want to make sure I help you with the right thing. \"\n",
    "                \"Could you clarify whether this is about industrial equipment, \"\n",
    "                \"cooking/recipes, or scientific research?\"\n",
    "            )\n",
    "\n",
    "    def _build_routing_from_result(self, result: Dict, query: str) -> Dict:\n",
    "        \"\"\"Common post-processing for LLM routing JSON.\"\"\"\n",
    "        domain = result.get(\"domain\", \"fallback\")\n",
    "        if domain not in self.valid_domains:\n",
    "            logger.warning(f\"Invalid domain '{domain}', using fallback\")\n",
    "            domain = \"fallback\"\n",
    "\n",
    "        # Validate and clamp confidence to [0, 1]\n",
    "        try:\n",
    "            confidence = float(result.get(\"confidence\", 0.0))\n",
    "        except (TypeError, ValueError):\n",
    "            confidence = 0.0\n",
    "        if not (0.0 <= confidence <= 1.0) or confidence != confidence:  # NaN check\n",
    "            logger.warning(f\"Confidence out of range ({confidence}), clamping\")\n",
    "            confidence = max(0.0, min(1.0, confidence)) if confidence == confidence else 0.0\n",
    "\n",
    "        if confidence < self.threshold and domain not in (\"clarify\", \"fallback\"):\n",
    "            logger.info(\n",
    "                f\"Low confidence ({confidence:.2f} < {self.threshold}), \"\n",
    "                f\"switching to clarification\"\n",
    "            )\n",
    "            domain = \"clarify\"\n",
    "\n",
    "        routing = {\n",
    "            \"domain\": domain,\n",
    "            \"confidence\": confidence,\n",
    "            \"reasoning\": result.get(\"reasoning\", \"\"),\n",
    "        }\n",
    "\n",
    "        logger.info(\n",
    "            f\"Routed to '{domain}' (confidence={confidence:.2f}): \"\n",
    "            f\"{routing['reasoning'][:60]}\"\n",
    "        )\n",
    "        return routing\n",
    "\n",
    "    def _route_with_simple_prompt(self, query: str) -> Dict | None:\n",
    "        \"\"\"\n",
    "        Backup routing path with a much simpler JSON-only prompt.\n",
    "\n",
    "        Used when the rich supervisor prompt fails to produce valid JSON.\n",
    "        \"\"\"\n",
    "        prompt = (\n",
    "            \"You are a router for a multi-domain assistant.\\n\"\n",
    "            \"Given a user query, choose exactly one domain from:\\n\"\n",
    "            '  - \"industrial\"\\n'\n",
    "            '  - \"recipe\"\\n'\n",
    "            '  - \"scientific\"\\n'\n",
    "            '  - \"clarify\"\\n'\n",
    "            '  - \"fallback\"\\n\\n'\n",
    "            \"Respond ONLY with valid JSON in this format:\\n\"\n",
    "            '{\\n'\n",
    "            '  \\\"domain\\\": \\\"<one of the above>\\\",\\n'\n",
    "            \"  \\\"confidence\\\": <0.0-1.0>,\\n\"\n",
    "            '  \\\"reasoning\\\": \\\"<brief explanation>\\\"\\n'\n",
    "            \"}\\n\\n\"\n",
    "            f'User query: \\\"{query}\\\"'\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = self.llm.invoke_and_parse_json([HumanMessage(content=prompt)])\n",
    "            if \"error\" in result:\n",
    "                logger.warning(\n",
    "                    \"Simple supervisor prompt parse failed, raw: %s\",\n",
    "                    result.get(\"raw\", \"\")[:120],\n",
    "                )\n",
    "                return None\n",
    "            return self._build_routing_from_result(result, query)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Simple supervisor routing failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _fallback_routing(query: str) -> Dict:\n",
    "        \"\"\"Fallback when LLM routing fails entirely.\"\"\"\n",
    "        return {\n",
    "            \"domain\": \"fallback\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"reasoning\": \"Routing failed â€” using web search fallback\",\n",
    "        }\n",
    "''',\n",
    "\n",
    "    'orchestration/workflow_graph.py': r'''\n",
    "\"\"\"\n",
    "Workflow Graph\n",
    "==============\n",
    "LangGraph StateGraph that wires the supervisor, agents, and CRAG pipeline\n",
    "into a complete executable workflow.\n",
    "\n",
    "    START â†’ supervisor_node â†’ route_decision\n",
    "        â”œâ”€ industrial â†’ industrial_node â†’ finalize\n",
    "        â”œâ”€ recipe â†’ recipe_node â†’ finalize\n",
    "        â”œâ”€ scientific â†’ scientific_node â†’ finalize\n",
    "        â”œâ”€ clarify â†’ clarify_node â†’ END\n",
    "        â””â”€ fallback â†’ fallback_node â†’ finalize\n",
    "    finalize â†’ END\n",
    "\n",
    "Usage:\n",
    "    from orchestration.workflow_graph import build_workflow, run_query\n",
    "    workflow = build_workflow(llm, vector_store, index_builder)\n",
    "    result = run_query(workflow, \"My PLC is showing fault F0003\")\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import importlib\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "from orchestration.state_schema import AgentState, create_initial_state\n",
    "from orchestration.supervisor import SupervisorAgent\n",
    "from rag_core.crag_pipeline import CRAGPipeline\n",
    "from foundation.vector_store import VectorStoreService\n",
    "from ingestion.index_builder import IndexBuilder\n",
    "from config.settings import CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _import_class(dotted_path: str):\n",
    "    \"\"\"Import a class from a dotted path like 'agents.industrial_agent.IndustrialAgent'.\"\"\"\n",
    "    module_path, class_name = dotted_path.rsplit(\".\", 1)\n",
    "    module = importlib.import_module(module_path)\n",
    "    return getattr(module, class_name)\n",
    "\n",
    "\n",
    "# â”€â”€ Node Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Each node is a function: State â†’ State\n",
    "\n",
    "def _make_supervisor_node(supervisor: SupervisorAgent):\n",
    "    \"\"\"Create the supervisor routing node.\"\"\"\n",
    "    def supervisor_node(state: AgentState) -> AgentState:\n",
    "        query = state[\"user_query\"]\n",
    "        t0 = time.perf_counter()\n",
    "        routing = supervisor.route(query)\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        state[\"timing_supervisor_s\"] = elapsed\n",
    "        logger.info(\"â± Supervisor (route): %.1fs\", elapsed)\n",
    "\n",
    "        state[\"routed_domain\"] = routing[\"domain\"]\n",
    "        state[\"routing_confidence\"] = routing[\"confidence\"]\n",
    "        state[\"routing_reasoning\"] = routing[\"reasoning\"]\n",
    "        state[\"status\"] = \"routing\"\n",
    "\n",
    "        return state\n",
    "    return supervisor_node\n",
    "\n",
    "\n",
    "def _make_agent_node(agent, domain_name: str):\n",
    "    \"\"\"Create a specialist agent execution node.\"\"\"\n",
    "    def agent_node(state: AgentState) -> AgentState:\n",
    "        query = state[\"user_query\"]\n",
    "        state[\"status\"] = \"processing\"\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        result = agent.handle(query)\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        state[\"timing_agent_s\"] = elapsed\n",
    "        state[\"timing_crag_breakdown\"] = getattr(result, \"timing_breakdown\", {})\n",
    "        logger.info(\"â± Agent (%s): %.1fs\", domain_name, elapsed)\n",
    "\n",
    "        state[\"agent_response\"] = result.response\n",
    "        state[\"agent_sources\"] = result.sources\n",
    "        state[\"agent_confidence\"] = result.confidence\n",
    "        state[\"agent_grounded\"] = result.grounded\n",
    "        state[\"escalated\"] = result.escalated\n",
    "        state[\"escalation_reason\"] = result.escalation_reason\n",
    "\n",
    "        return state\n",
    "    return agent_node\n",
    "\n",
    "\n",
    "def _make_clarify_node(supervisor: SupervisorAgent):\n",
    "    \"\"\"Create the clarification node.\"\"\"\n",
    "    def clarify_node(state: AgentState) -> AgentState:\n",
    "        query = state[\"user_query\"]\n",
    "        routing = {\n",
    "            \"confidence\": state.get(\"routing_confidence\", 0),\n",
    "            \"reasoning\": state.get(\"routing_reasoning\", \"\"),\n",
    "        }\n",
    "        t0 = time.perf_counter()\n",
    "        clarification = supervisor.generate_clarification(query, routing)\n",
    "        state[\"timing_clarify_s\"] = time.perf_counter() - t0\n",
    "        logger.info(\"â± Clarify (LLM): %.1fs\", state[\"timing_clarify_s\"])\n",
    "\n",
    "        state[\"needs_clarification\"] = True\n",
    "        state[\"clarification_message\"] = clarification\n",
    "        state[\"final_response\"] = clarification\n",
    "        state[\"status\"] = \"complete\"\n",
    "\n",
    "        return state\n",
    "    return clarify_node\n",
    "\n",
    "\n",
    "def _make_fallback_node(llm):\n",
    "    \"\"\"Create the web-search fallback node.\"\"\"\n",
    "    def fallback_node(state: AgentState) -> AgentState:\n",
    "        # In a full implementation, this would use Tavily Search API\n",
    "        state[\"final_response\"] = (\n",
    "            \"This question falls outside my specialized domains \"\n",
    "            \"(industrial troubleshooting, recipes, and scientific papers). \"\n",
    "            \"I'd recommend searching the web for more information on this topic.\"\n",
    "        )\n",
    "        state[\"status\"] = \"complete\"\n",
    "        return state\n",
    "    return fallback_node\n",
    "\n",
    "\n",
    "def finalize_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Finalize the response â€” use agent output or escalation message.\"\"\"\n",
    "    if state.get(\"escalated\") and not state.get(\"agent_response\"):\n",
    "        state[\"final_response\"] = (\n",
    "            f\"I encountered difficulty answering your question. \"\n",
    "            f\"Reason: {state.get('escalation_reason', 'Unknown')}. \"\n",
    "            f\"Please try rephrasing or providing more details.\"\n",
    "        )\n",
    "    else:\n",
    "        response = state.get(\"agent_response\", \"\")\n",
    "        # Add confidence warning if needed\n",
    "        if state.get(\"escalated\"):\n",
    "            response += (\n",
    "                \"\\n\\nâš ï¸ *Note: I'm not fully confident in this answer. \"\n",
    "                \"Please verify the information from the referenced sources.*\"\n",
    "            )\n",
    "        state[\"final_response\"] = response\n",
    "\n",
    "    state[\"status\"] = \"complete\"\n",
    "    return state\n",
    "\n",
    "\n",
    "# â”€â”€ Routing Logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def _route_decision(state: AgentState) -> str:\n",
    "    \"\"\"Conditional edge: route to the appropriate agent node.\"\"\"\n",
    "    domain = state.get(\"routed_domain\", \"fallback\")\n",
    "\n",
    "    valid_routes = {spec.name for spec in CONFIG.supervisor.domain_registry}\n",
    "    valid_routes |= {\"clarify\", \"fallback\"}\n",
    "    if domain not in valid_routes:\n",
    "        logger.warning(f\"Unknown domain '{domain}', routing to fallback\")\n",
    "        return \"fallback\"\n",
    "\n",
    "    return domain\n",
    "\n",
    "\n",
    "# â”€â”€ Graph Builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def build_workflow(\n",
    "    llm,\n",
    "    vector_store: VectorStoreService,\n",
    "    index_builder: Optional[IndexBuilder] = None,\n",
    ") -> StateGraph:\n",
    "    \"\"\"\n",
    "    Build and compile the complete LangGraph workflow.\n",
    "\n",
    "    Args:\n",
    "        llm: LangChain-compatible chat model.\n",
    "        vector_store: Initialized VectorStoreService.\n",
    "        index_builder: Optional IndexBuilder for on-demand ArXiv fetching.\n",
    "\n",
    "    Returns:\n",
    "        Compiled LangGraph StateGraph ready for invocation.\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    supervisor = SupervisorAgent(llm)\n",
    "    crag = CRAGPipeline(llm, vector_store)\n",
    "\n",
    "    # Build graph\n",
    "    graph = StateGraph(AgentState)\n",
    "    graph.add_node(\"supervisor\", _make_supervisor_node(supervisor))\n",
    "\n",
    "    # Wire domain agents from config registry (config-driven extensibility)\n",
    "    routing_map: Dict[str, str] = {}\n",
    "    for spec in CONFIG.supervisor.domain_registry:\n",
    "        agent_cls = _import_class(spec.agent_class)\n",
    "        # ScientificAgent accepts an optional index_builder\n",
    "        if spec.name == \"scientific\" and index_builder is not None:\n",
    "            agent_instance = agent_cls(crag, index_builder)\n",
    "        else:\n",
    "            agent_instance = agent_cls(crag)\n",
    "        graph.add_node(spec.name, _make_agent_node(agent_instance, spec.name))\n",
    "        routing_map[spec.name] = spec.name\n",
    "\n",
    "    graph.add_node(\"clarify\", _make_clarify_node(supervisor))\n",
    "    graph.add_node(\"fallback\", _make_fallback_node(llm))\n",
    "    graph.add_node(\"finalize\", finalize_node)\n",
    "\n",
    "    graph.set_entry_point(\"supervisor\")\n",
    "\n",
    "    routing_map[\"clarify\"] = \"clarify\"\n",
    "    routing_map[\"fallback\"] = \"fallback\"\n",
    "    graph.add_conditional_edges(\"supervisor\", _route_decision, routing_map)\n",
    "\n",
    "    # Domain agent nodes â†’ finalize\n",
    "    for spec in CONFIG.supervisor.domain_registry:\n",
    "        graph.add_edge(spec.name, \"finalize\")\n",
    "    graph.add_edge(\"fallback\", \"finalize\")\n",
    "\n",
    "    # Clarify and finalize â†’ END\n",
    "    graph.add_edge(\"clarify\", END)\n",
    "    graph.add_edge(\"finalize\", END)\n",
    "\n",
    "    # Compile\n",
    "    compiled = graph.compile()\n",
    "    logger.info(\"Workflow graph compiled successfully\")\n",
    "    return compiled\n",
    "\n",
    "\n",
    "# â”€â”€ Convenience Runner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def run_query(workflow, query: str, history=None) -> Dict:\n",
    "    \"\"\"\n",
    "    Run a query through the full workflow and return results.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"response\": str,\n",
    "            \"domain\": str,\n",
    "            \"confidence\": float,\n",
    "            \"sources\": list,\n",
    "            \"escalated\": bool,\n",
    "            \"needs_clarification\": bool,\n",
    "            \"timing\": { \"total_s\", \"supervisor_s\", \"agent_s\", \"crag\": {...} },\n",
    "        }\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    initial_state = create_initial_state(query, history)\n",
    "    final_state = workflow.invoke(initial_state)\n",
    "    total_s = time.perf_counter() - t0\n",
    "\n",
    "    crag = final_state.get(\"timing_crag_breakdown\") or {}\n",
    "\n",
    "    timing = {\n",
    "        \"total_s\": total_s,\n",
    "        \"supervisor_s\": final_state.get(\"timing_supervisor_s\", 0.0),\n",
    "        \"agent_s\": final_state.get(\"timing_agent_s\", 0.0),\n",
    "        \"clarify_s\": final_state.get(\"timing_clarify_s\", 0.0),\n",
    "        \"crag\": {\n",
    "            \"retrieve_s\": crag.get(\"retrieve_s\", 0.0),\n",
    "            \"grade_s\": crag.get(\"grade_s\", 0.0),\n",
    "            \"rewrite_s\": crag.get(\"rewrite_s\", 0.0),\n",
    "            \"generate_s\": crag.get(\"generate_s\", 0.0),\n",
    "            \"validate_s\": crag.get(\"validate_s\", 0.0),\n",
    "            \"total_s\": crag.get(\"total_s\", 0.0),\n",
    "        },\n",
    "    }\n",
    "    logger.info(\"â± Total query: %.1fs (%.1f min)\", total_s, total_s / 60)\n",
    "\n",
    "    return {\n",
    "        \"response\": final_state.get(\"final_response\", \"\"),\n",
    "        \"domain\": final_state.get(\"routed_domain\", \"\"),\n",
    "        \"confidence\": final_state.get(\"routing_confidence\", 0.0),\n",
    "        \"sources\": final_state.get(\"agent_sources\", []),\n",
    "        \"escalated\": final_state.get(\"escalated\", False),\n",
    "        \"needs_clarification\": final_state.get(\"needs_clarification\", False),\n",
    "        \"status\": final_state.get(\"status\", \"unknown\"),\n",
    "        \"timing\": timing,\n",
    "    }\n",
    "''',\n",
    "\n",
    "    'rag_core/__init__.py': r'''\n",
    "''',\n",
    "\n",
    "    'rag_core/crag_pipeline.py': r'''\n",
    "\"\"\"\n",
    "CRAG Pipeline (Corrective RAG)\n",
    "===============================\n",
    "Orchestrates the full self-correcting retrieval-augmented generation loop:\n",
    "\n",
    "    Retrieve â†’ Grade â†’ Decide â†’ Generate â†’ Validate\n",
    "\n",
    "If retrieved docs are irrelevant, rewrites the query and retries\n",
    "(up to max_rewrite_attempts). If generation seems ungrounded,\n",
    "flags for escalation to the supervisor.\n",
    "\n",
    "This is the SHARED pipeline used by ALL specialized agents.\n",
    "\n",
    "Usage:\n",
    "    from rag_core.crag_pipeline import CRAGPipeline\n",
    "    pipeline = CRAGPipeline(llm=llm, vector_store=vs)\n",
    "    result = pipeline.run(query=\"PLC fault F0003\", domain=\"industrial\")\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from rag_core.retriever import Retriever\n",
    "from rag_core.document_grader import DocumentGrader\n",
    "from rag_core.query_rewriter import QueryRewriter\n",
    "from rag_core.response_generator import ResponseGenerator\n",
    "from rag_core.hallucination_checker import HallucinationChecker\n",
    "from foundation.vector_store import VectorStoreService\n",
    "from config.settings import CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CRAGResult:\n",
    "    \"\"\"Result of a CRAG pipeline execution.\"\"\"\n",
    "    response: str = \"\"\n",
    "    sources: list = field(default_factory=list)\n",
    "    domain: str = \"\"\n",
    "    grounded: bool = True\n",
    "    confidence: float = 1.0\n",
    "    attempts: int = 1\n",
    "    escalated: bool = False\n",
    "    escalation_reason: str = \"\"\n",
    "    query_history: list = field(default_factory=list)\n",
    "    grading_summary: dict = field(default_factory=dict)\n",
    "    timing_breakdown: dict = field(default_factory=dict)\n",
    "    per_attempt_timing: list = field(default_factory=list)  # [{retrieve_s, grade_s, rewrite_s}, ...]\n",
    "\n",
    "\n",
    "class CRAGPipeline:\n",
    "    \"\"\"\n",
    "    Self-correcting RAG pipeline implementing the CRAG pattern.\n",
    "\n",
    "    The pipeline:\n",
    "    1. RETRIEVE â€” semantic search in domain collection\n",
    "    2. GRADE â€” LLM judges document relevance\n",
    "    3. DECIDE â€”\n",
    "         â€¢ Relevant docs found â†’ proceed to generate\n",
    "         â€¢ No relevant docs â†’ rewrite query and retry (max N times)\n",
    "         â€¢ Still nothing â†’ escalate to supervisor\n",
    "    4. GENERATE â€” produce grounded response with source attribution\n",
    "    5. VALIDATE â€” hallucination check; escalate if ungrounded\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        vector_store: VectorStoreService,\n",
    "    ):\n",
    "        self.retriever = Retriever(vector_store)\n",
    "        self.grader = DocumentGrader(llm)\n",
    "        self.rewriter = QueryRewriter(llm)\n",
    "        self.generator = ResponseGenerator(llm)\n",
    "        self.checker = HallucinationChecker(llm)\n",
    "        self.max_attempts = CONFIG.rag.max_rewrite_attempts + 1  # 1 initial + N rewrites\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        query: str,\n",
    "        domain: str,\n",
    "        k: Optional[int] = None,\n",
    "    ) -> CRAGResult:\n",
    "        \"\"\"\n",
    "        Execute the full CRAG loop.\n",
    "\n",
    "        Args:\n",
    "            query: User query (or already rewritten query).\n",
    "            domain: Target domain for retrieval.\n",
    "            k: Number of documents to retrieve per attempt.\n",
    "\n",
    "        Returns:\n",
    "            CRAGResult with response, sources, and pipeline metadata.\n",
    "        \"\"\"\n",
    "        result = CRAGResult(domain=domain)\n",
    "        current_query = query\n",
    "        result.query_history.append(current_query)\n",
    "\n",
    "        for attempt in range(1, self.max_attempts + 1):\n",
    "            result.attempts = attempt\n",
    "            attempt_timing: dict = {}\n",
    "            logger.info(f\"CRAG attempt {attempt}/{self.max_attempts} | \"\n",
    "                        f\"Query: {current_query[:80]}\")\n",
    "\n",
    "            # â”€â”€ Step 1: Retrieve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            t0 = time.perf_counter()\n",
    "            retrieved_docs = self.retriever.retrieve(domain, current_query, k=k)\n",
    "            retrieve_elapsed = time.perf_counter() - t0\n",
    "            attempt_timing[\"retrieve_s\"] = retrieve_elapsed\n",
    "            result.timing_breakdown[\"retrieve_s\"] = result.timing_breakdown.get(\"retrieve_s\", 0) + retrieve_elapsed\n",
    "\n",
    "            if not retrieved_docs:\n",
    "                logger.warning(\"No documents retrieved at all\")\n",
    "                if attempt < self.max_attempts:\n",
    "                    t0 = time.perf_counter()\n",
    "                    current_query = self._rewrite_and_retry(\n",
    "                        current_query, domain, \"No documents returned\"\n",
    "                    )\n",
    "                    rewrite_elapsed = time.perf_counter() - t0\n",
    "                    attempt_timing[\"rewrite_s\"] = rewrite_elapsed\n",
    "                    result.timing_breakdown[\"rewrite_s\"] = result.timing_breakdown.get(\"rewrite_s\", 0) + rewrite_elapsed\n",
    "                    result.per_attempt_timing.append(attempt_timing)\n",
    "                    result.query_history.append(current_query)\n",
    "                    continue\n",
    "                else:\n",
    "                    result.per_attempt_timing.append(attempt_timing)\n",
    "                    return self._escalate(\n",
    "                        result, \"No documents found after all attempts\"\n",
    "                    )\n",
    "\n",
    "            # â”€â”€ Step 2: Grade â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            t0 = time.perf_counter()\n",
    "            grading = self.grader.grade_documents(current_query, retrieved_docs)\n",
    "            grade_elapsed = time.perf_counter() - t0\n",
    "            attempt_timing[\"grade_s\"] = grade_elapsed\n",
    "            result.timing_breakdown[\"grade_s\"] = result.timing_breakdown.get(\"grade_s\", 0) + grade_elapsed\n",
    "            result.grading_summary = {\n",
    "                \"relevant\": len(grading[\"relevant\"]),\n",
    "                \"irrelevant\": len(grading[\"irrelevant\"]),\n",
    "                \"ambiguous\": len(grading[\"ambiguous\"]),\n",
    "            }\n",
    "\n",
    "            # â”€â”€ Step 3: Decide â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            if self.grader.has_sufficient_context(grading):\n",
    "                context_docs = grading[\"relevant\"] + grading[\"ambiguous\"]\n",
    "                logger.info(\n",
    "                    f\"Sufficient context found: {len(context_docs)} docs\"\n",
    "                )\n",
    "                result.per_attempt_timing.append(attempt_timing)\n",
    "                break\n",
    "            else:\n",
    "                logger.info(\"Insufficient relevant documents\")\n",
    "                if attempt < self.max_attempts:\n",
    "                    failure_context = (\n",
    "                        f\"Retrieved {len(retrieved_docs)} docs but \"\n",
    "                        f\"{len(grading['irrelevant'])} were irrelevant. \"\n",
    "                        f\"Grades: {result.grading_summary}\"\n",
    "                    )\n",
    "                    t0 = time.perf_counter()\n",
    "                    current_query = self._rewrite_and_retry(\n",
    "                        current_query, domain, failure_context\n",
    "                    )\n",
    "                    rewrite_elapsed = time.perf_counter() - t0\n",
    "                    attempt_timing[\"rewrite_s\"] = rewrite_elapsed\n",
    "                    result.timing_breakdown[\"rewrite_s\"] = result.timing_breakdown.get(\"rewrite_s\", 0) + rewrite_elapsed\n",
    "                    result.per_attempt_timing.append(attempt_timing)\n",
    "                    result.query_history.append(current_query)\n",
    "                    continue\n",
    "                else:\n",
    "                    result.per_attempt_timing.append(attempt_timing)\n",
    "                    return self._escalate(\n",
    "                        result,\n",
    "                        \"Could not find relevant documents after \"\n",
    "                        f\"{self.max_attempts} attempts\"\n",
    "                    )\n",
    "        else:\n",
    "            return self._escalate(result, \"Max attempts exhausted\")\n",
    "\n",
    "        # â”€â”€ Step 4: Generate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        t0 = time.perf_counter()\n",
    "        generation = self.generator.generate(query, context_docs, domain)\n",
    "        result.timing_breakdown[\"generate_s\"] = time.perf_counter() - t0\n",
    "        result.response = generation[\"response\"]\n",
    "        result.sources = generation[\"sources\"]\n",
    "\n",
    "        # â”€â”€ Step 5: Validate (optional; skip for faster inference) â”€â”€â”€â”€â”€\n",
    "        if CONFIG.rag.skip_hallucination_check:\n",
    "            result.grounded = True\n",
    "            result.confidence = 1.0\n",
    "            result.timing_breakdown[\"validate_s\"] = 0.0\n",
    "            logger.info(\"Hallucination check skipped (skip_hallucination_check=True)\")\n",
    "        else:\n",
    "            t0 = time.perf_counter()\n",
    "            validation = self.checker.check(result.response, context_docs)\n",
    "            result.timing_breakdown[\"validate_s\"] = time.perf_counter() - t0\n",
    "            result.grounded = validation[\"grounded\"]\n",
    "            result.confidence = validation[\"confidence\"]\n",
    "            if validation[\"should_escalate\"]:\n",
    "                logger.warning(\n",
    "                    f\"Hallucination detected: {validation['issues']}\"\n",
    "                )\n",
    "                result.escalated = True\n",
    "                result.escalation_reason = (\n",
    "                    f\"Response may not be fully grounded. \"\n",
    "                    f\"Issues: {validation['issues']}\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"Response validated (confidence={result.confidence:.2f})\"\n",
    "                )\n",
    "\n",
    "        result.timing_breakdown[\"total_s\"] = (\n",
    "            result.timing_breakdown.get(\"retrieve_s\", 0)\n",
    "            + result.timing_breakdown.get(\"grade_s\", 0)\n",
    "            + result.timing_breakdown.get(\"rewrite_s\", 0)\n",
    "            + result.timing_breakdown.get(\"generate_s\", 0)\n",
    "            + result.timing_breakdown.get(\"validate_s\", 0)\n",
    "        )\n",
    "        logger.info(\n",
    "            \"â± CRAG timing: retrieve=%.1fs grade=%.1fs rewrite=%.1fs \"\n",
    "            \"generate=%.1fs validate=%.1fs total=%.1fs (attempts=%d)\",\n",
    "            result.timing_breakdown.get(\"retrieve_s\", 0),\n",
    "            result.timing_breakdown.get(\"grade_s\", 0),\n",
    "            result.timing_breakdown.get(\"rewrite_s\", 0),\n",
    "            result.timing_breakdown.get(\"generate_s\", 0),\n",
    "            result.timing_breakdown.get(\"validate_s\", 0),\n",
    "            result.timing_breakdown[\"total_s\"],\n",
    "            result.attempts,\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def _rewrite_and_retry(\n",
    "        self, query: str, domain: str, failure_context: str\n",
    "    ) -> str:\n",
    "        \"\"\"Rewrite query for the next retrieval attempt.\"\"\"\n",
    "        new_query = self.rewriter.rewrite(query, domain, failure_context)\n",
    "        logger.info(f\"Rewritten: '{query[:50]}' â†’ '{new_query[:50]}'\")\n",
    "        return new_query\n",
    "\n",
    "    @staticmethod\n",
    "    def _escalate(result: CRAGResult, reason: str) -> CRAGResult:\n",
    "        \"\"\"Mark the result as needing supervisor escalation.\"\"\"\n",
    "        result.escalated = True\n",
    "        result.escalation_reason = reason\n",
    "        result.response = (\n",
    "            \"I wasn't able to find relevant information to answer your question \"\n",
    "            \"from my knowledge base. Let me escalate this for further assistance.\"\n",
    "        )\n",
    "        logger.warning(f\"Escalating to supervisor: {reason}\")\n",
    "        return result\n",
    "''',\n",
    "\n",
    "    'rag_core/document_grader.py': r'''\n",
    "\"\"\"\n",
    "Document Grader\n",
    "===============\n",
    "LLM-as-judge that evaluates whether retrieved documents are relevant\n",
    "to the user's query. Core component of the CRAG self-correction loop.\n",
    "\n",
    "Usage:\n",
    "    from rag_core.document_grader import DocumentGrader\n",
    "    grader = DocumentGrader(llm=llm)\n",
    "    results = grader.grade_documents(query, retrieved_docs)\n",
    "    relevant_docs = results[\"relevant\"]\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from config.prompts import GRADER_PROMPT, GRADER_BATCH_PROMPT\n",
    "from config.settings import CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DocumentGrader:\n",
    "    \"\"\"\n",
    "    Grades retrieved documents for relevance using LLM-as-judge.\n",
    "    Categorizes each document as relevant, irrelevant, or ambiguous.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            llm: A LangChain-compatible chat model (e.g., KerasHubChatModel).\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.threshold = CONFIG.rag.relevance_threshold\n",
    "\n",
    "    def grade_single(self, query: str, document: Document) -> dict:\n",
    "        \"\"\"\n",
    "        Grade a single document's relevance to the query.\n",
    "\n",
    "        Returns:\n",
    "            {\"relevance\": \"relevant\"|\"irrelevant\"|\"ambiguous\",\n",
    "             \"score\": float, \"reasoning\": str}\n",
    "        \"\"\"\n",
    "        prompt = GRADER_PROMPT.format(\n",
    "            query=query,\n",
    "            document=document.page_content[:2000],  # truncate for context window\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = self.llm.invoke_and_parse_json(\n",
    "                [HumanMessage(content=prompt)]\n",
    "            )\n",
    "            if \"error\" in result:\n",
    "                # Parse failed â€” fall back to score-based grading\n",
    "                logger.warning(\"Grader JSON parse failed, using fallback\")\n",
    "                return {\n",
    "                    \"relevance\": \"ambiguous\",\n",
    "                    \"score\": 0.5,\n",
    "                    \"reasoning\": \"Parse error â€” defaulting to ambiguous\",\n",
    "                }\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Grading failed: {e}\")\n",
    "            return {\n",
    "                \"relevance\": \"ambiguous\",\n",
    "                \"score\": 0.5,\n",
    "                \"reasoning\": f\"Error: {str(e)}\",\n",
    "            }\n",
    "\n",
    "    def grade_documents_batch(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[Document],\n",
    "    ) -> Dict[str, List[Document]]:\n",
    "        \"\"\"\n",
    "        Grade all documents in a single LLM call (faster than N separate calls).\n",
    "        Falls back to per-document grading if batch parsing fails.\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return {\"relevant\": [], \"irrelevant\": [], \"ambiguous\": [], \"grades\": []}\n",
    "\n",
    "        documents_block = \"\\n\\n---\\n\\n\".join(\n",
    "            f\"[Document {i}]\\n{doc.page_content[:2000]}\"\n",
    "            for i, doc in enumerate(documents)\n",
    "        )\n",
    "        prompt = GRADER_BATCH_PROMPT.format(\n",
    "            query=query,\n",
    "            documents_block=documents_block,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = self.llm.invoke_and_parse_json([HumanMessage(content=prompt)])\n",
    "            if \"error\" in result or \"grades\" not in result:\n",
    "                raise ValueError(\"Batch grader returned no grades\")\n",
    "            raw_grades = result[\"grades\"]\n",
    "            if not isinstance(raw_grades, list) or len(raw_grades) != len(documents):\n",
    "                raise ValueError(\n",
    "                    f\"Expected {len(documents)} grades, got {len(raw_grades) if isinstance(raw_grades, list) else 'non-list'}\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Batch grading failed ({e}), falling back to per-document grading\")\n",
    "            return self._grade_documents_sequential(query, documents)\n",
    "\n",
    "        buckets = {\"relevant\": [], \"irrelevant\": [], \"ambiguous\": []}\n",
    "        grades = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            g = raw_grades[i] if i < len(raw_grades) else {}\n",
    "            if not isinstance(g, dict):\n",
    "                g = {\"relevance\": \"ambiguous\", \"score\": 0.5, \"reasoning\": \"Invalid grade\"}\n",
    "            relevance = g.get(\"relevance\", \"ambiguous\")\n",
    "            if relevance not in CONFIG.rag.grading_labels:\n",
    "                relevance = \"ambiguous\"\n",
    "            buckets[relevance].append(doc)\n",
    "            grades.append({\"doc_index\": i, \"grade\": g})\n",
    "            logger.debug(f\"Doc {i}: {relevance} (score={g.get('score', '?')})\")\n",
    "\n",
    "        logger.info(\n",
    "            f\"Grading results â€” relevant: {len(buckets['relevant'])}, \"\n",
    "            f\"irrelevant: {len(buckets['irrelevant'])}, \"\n",
    "            f\"ambiguous: {len(buckets['ambiguous'])}\"\n",
    "        )\n",
    "        return {**buckets, \"grades\": grades}\n",
    "\n",
    "    def _grade_documents_sequential(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[Document],\n",
    "    ) -> Dict[str, List[Document]]:\n",
    "        \"\"\"Per-document grading (slower, used as fallback).\"\"\"\n",
    "        buckets = {\"relevant\": [], \"irrelevant\": [], \"ambiguous\": []}\n",
    "        grades = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            grade = self.grade_single(query, doc)\n",
    "            relevance = grade.get(\"relevance\", \"ambiguous\")\n",
    "            if relevance not in CONFIG.rag.grading_labels:\n",
    "                relevance = \"ambiguous\"\n",
    "            buckets[relevance].append(doc)\n",
    "            grades.append({\"doc_index\": i, \"grade\": grade})\n",
    "        logger.info(\n",
    "            f\"Grading results â€” relevant: {len(buckets['relevant'])}, \"\n",
    "            f\"irrelevant: {len(buckets['irrelevant'])}, ambiguous: {len(buckets['ambiguous'])}\"\n",
    "        )\n",
    "        return {**buckets, \"grades\": grades}\n",
    "\n",
    "    def grade_documents(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[Document],\n",
    "    ) -> Dict[str, List[Document]]:\n",
    "        \"\"\"\n",
    "        Grade all retrieved documents and bucket them.\n",
    "        Uses a single batch LLM call when possible for much faster inference.\n",
    "        Returns:\n",
    "            {\n",
    "                \"relevant\": [Document, ...],\n",
    "                \"irrelevant\": [Document, ...],\n",
    "                \"ambiguous\": [Document, ...],\n",
    "                \"grades\": [{\"doc_index\": int, \"grade\": dict}, ...],\n",
    "            }\n",
    "        \"\"\"\n",
    "        return self.grade_documents_batch(query, documents)\n",
    "\n",
    "    def has_sufficient_context(self, grading_result: dict) -> bool:\n",
    "        \"\"\"Check if enough relevant documents were found to generate a response.\"\"\"\n",
    "        return len(grading_result[\"relevant\"]) >= 1\n",
    "''',\n",
    "\n",
    "    'rag_core/hallucination_checker.py': r'''\n",
    "\"\"\"\n",
    "Hallucination Checker\n",
    "=====================\n",
    "Validates that a generated response is grounded in its source documents.\n",
    "Flags unsupported claims and provides a confidence score.\n",
    "\n",
    "Usage:\n",
    "    from rag_core.hallucination_checker import HallucinationChecker\n",
    "    checker = HallucinationChecker(llm=llm)\n",
    "    result = checker.check(response_text, source_docs)\n",
    "    if result[\"grounded\"]:\n",
    "        # Safe to return to user\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from config.prompts import HALLUCINATION_CHECK_SYSTEM, HALLUCINATION_CHECK_USER\n",
    "from config.settings import CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class HallucinationChecker:\n",
    "    \"\"\"Validates response grounding against source documents.\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.confidence_threshold = CONFIG.rag.confidence_threshold\n",
    "\n",
    "    def check(\n",
    "        self,\n",
    "        response: str,\n",
    "        source_documents: List[Document],\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Check if the response is grounded in the source documents.\n",
    "\n",
    "        Returns:\n",
    "            {\n",
    "                \"grounded\": bool,\n",
    "                \"confidence\": float,\n",
    "                \"issues\": list[str],\n",
    "                \"should_escalate\": bool,\n",
    "            }\n",
    "        \"\"\"\n",
    "        sources_text = \"\\n\\n---\\n\\n\".join(\n",
    "            f\"[Source {i+1}]\\n{doc.page_content[:1500]}\"\n",
    "            for i, doc in enumerate(source_documents)\n",
    "        )\n",
    "\n",
    "        user_content = HALLUCINATION_CHECK_USER.format(\n",
    "            sources=sources_text,\n",
    "            response=response[:3000],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = self.llm.invoke_and_parse_json([\n",
    "                SystemMessage(content=HALLUCINATION_CHECK_SYSTEM),\n",
    "                HumanMessage(content=user_content),\n",
    "            ])\n",
    "            if \"error\" in result:\n",
    "                logger.warning(\"Hallucination check parse failed\")\n",
    "                return self._default_result()\n",
    "\n",
    "            grounded = result.get(\"grounded\", False)\n",
    "            confidence = float(result.get(\"confidence\", 0.5))\n",
    "            issues = result.get(\"issues\", [])\n",
    "\n",
    "            return {\n",
    "                \"grounded\": grounded,\n",
    "                \"confidence\": confidence,\n",
    "                \"issues\": issues,\n",
    "                \"should_escalate\": not grounded or confidence < self.confidence_threshold,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Hallucination check failed: {e}\")\n",
    "            return self._default_result()\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_result() -> dict:\n",
    "        \"\"\"Fail-safe: assume not fully grounded when check fails.\"\"\"\n",
    "        return {\n",
    "            \"grounded\": False,\n",
    "            \"confidence\": 0.5,\n",
    "            \"issues\": [\"Hallucination check could not be completed\"],\n",
    "            \"should_escalate\": True,\n",
    "        }\n",
    "''',\n",
    "\n",
    "    'rag_core/query_rewriter.py': r'''\n",
    "\"\"\"\n",
    "Query Rewriter\n",
    "==============\n",
    "Rewrites queries that failed to retrieve relevant documents.\n",
    "Uses the LLM to add domain-specific terms, expand abbreviations,\n",
    "or decompose compound questions.\n",
    "\n",
    "Usage:\n",
    "    from rag_core.query_rewriter import QueryRewriter\n",
    "    rewriter = QueryRewriter(llm=llm)\n",
    "    new_query = rewriter.rewrite(query, domain=\"industrial\", failure_context=\"...\")\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from config.prompts import REWRITER_PROMPT\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class QueryRewriter:\n",
    "    \"\"\"Rewrites queries to improve retrieval on retry.\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def rewrite(\n",
    "        self,\n",
    "        query: str,\n",
    "        domain: str = \"general\",\n",
    "        failure_context: str = \"No relevant documents found.\",\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Rewrite a query for better retrieval.\n",
    "\n",
    "        Args:\n",
    "            query: The original (or previously rewritten) query.\n",
    "            domain: Target domain for terminology hints.\n",
    "            failure_context: Why the previous retrieval failed.\n",
    "\n",
    "        Returns:\n",
    "            The rewritten query string.\n",
    "        \"\"\"\n",
    "        prompt = REWRITER_PROMPT.format(\n",
    "            query=query,\n",
    "            domain=domain,\n",
    "            failure_context=failure_context,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            rewritten = result.content.strip()\n",
    "            # Sanity check â€” don't return empty or overly long rewrites\n",
    "            if not rewritten or len(rewritten) > len(query) * 5:\n",
    "                logger.warning(\"Rewrite was empty or too long, keeping original\")\n",
    "                return query\n",
    "            logger.info(f\"Query rewritten: '{query[:60]}' â†’ '{rewritten[:60]}'\")\n",
    "            return rewritten\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Query rewrite failed: {e}\")\n",
    "            return query\n",
    "''',\n",
    "\n",
    "    'rag_core/response_generator.py': r'''\n",
    "\"\"\"\n",
    "Response Generator\n",
    "==================\n",
    "Produces grounded responses from query + relevant documents.\n",
    "Injects domain-specific system prompts and formats source context.\n",
    "\n",
    "Usage:\n",
    "    from rag_core.response_generator import ResponseGenerator\n",
    "    generator = ResponseGenerator(llm=llm)\n",
    "    response = generator.generate(query, relevant_docs, domain=\"industrial\")\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "import config.prompts as prompt_module\n",
    "from config.settings import CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _build_domain_prompts() -> dict:\n",
    "    \"\"\"Build domainâ†’prompt map from the config registry, falling back to legacy names.\"\"\"\n",
    "    prompts = {}\n",
    "    for spec in CONFIG.supervisor.domain_registry:\n",
    "        template = getattr(prompt_module, spec.prompt_key, None)\n",
    "        if template:\n",
    "            prompts[spec.name] = template\n",
    "        else:\n",
    "            logger.warning(\"No prompt template '%s' for domain '%s'\", spec.prompt_key, spec.name)\n",
    "    return prompts\n",
    "\n",
    "\n",
    "_DOMAIN_PROMPTS = _build_domain_prompts()\n",
    "\n",
    "\n",
    "class ResponseGenerator:\n",
    "    \"\"\"Generates grounded responses using retrieved context.\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[Document],\n",
    "        domain: str,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a response grounded in the provided documents.\n",
    "\n",
    "        Args:\n",
    "            query: User's original query.\n",
    "            documents: Relevant documents from retrieval.\n",
    "            domain: Target domain for prompt selection.\n",
    "\n",
    "        Returns:\n",
    "            {\n",
    "                \"response\": str,\n",
    "                \"sources\": [{\"source\": str, \"chunk_index\": int}, ...],\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Format context from documents\n",
    "        context = self._format_context(documents)\n",
    "        sources = self._extract_sources(documents)\n",
    "\n",
    "        # Select domain prompt\n",
    "        prompt_template = _DOMAIN_PROMPTS.get(domain)\n",
    "        if not prompt_template:\n",
    "            logger.warning(f\"No prompt for domain '{domain}', using generic\")\n",
    "            prompt_template = (\n",
    "                \"Answer the user's query using ONLY the provided context.\\n\\n\"\n",
    "                \"Retrieved Context:\\n{context}\\n\\nUser Query: {query}\"\n",
    "            )\n",
    "\n",
    "        prompt = prompt_template.format(context=context, query=query)\n",
    "\n",
    "        try:\n",
    "            result = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            response_text = result.content.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation failed: {e}\")\n",
    "            response_text = (\n",
    "                \"I encountered an error generating a response. \"\n",
    "                \"Please try rephrasing your question.\"\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"response\": response_text,\n",
    "            \"sources\": sources,\n",
    "            \"domain\": domain,\n",
    "            \"num_context_docs\": len(documents),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_context(documents: List[Document]) -> str:\n",
    "        \"\"\"Format documents into a numbered context string.\"\"\"\n",
    "        parts = []\n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            source = doc.metadata.get(\"source\", \"unknown\")\n",
    "            parts.append(f\"[Document {i} | Source: {source}]\\n{doc.page_content}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_sources(documents: List[Document]) -> list:\n",
    "        \"\"\"Extract source attribution from document metadata.\"\"\"\n",
    "        sources = []\n",
    "        for doc in documents:\n",
    "            sources.append({\n",
    "                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "                \"chunk_index\": doc.metadata.get(\"chunk_index\", -1),\n",
    "                \"parent_id\": doc.metadata.get(\"parent_id\", \"\"),\n",
    "                \"similarity_score\": doc.metadata.get(\"similarity_score\", 0.0),\n",
    "            })\n",
    "        return sources\n",
    "''',\n",
    "\n",
    "    'rag_core/retriever.py': r'''\n",
    "\"\"\"\n",
    "Retriever\n",
    "=========\n",
    "Thin wrapper around VectorStoreService.search() that provides a consistent\n",
    "interface for the CRAG pipeline. Adds logging and query preprocessing.\n",
    "\n",
    "Usage:\n",
    "    from rag_core.retriever import Retriever\n",
    "    retriever = Retriever(vector_store=vs)\n",
    "    docs = retriever.retrieve(\"industrial\", \"PLC fault F0003\", k=5)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from foundation.vector_store import VectorStoreService\n",
    "from config.settings import CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    \"\"\"\n",
    "    Domain-aware document retriever.\n",
    "    Routes queries to the correct vector store collection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStoreService):\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "    def retrieve(\n",
    "        self,\n",
    "        domain: str,\n",
    "        query: str,\n",
    "        k: Optional[int] = None,\n",
    "        filters: Optional[dict] = None,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k documents for a query from the specified domain.\n",
    "\n",
    "        Args:\n",
    "            domain: Target domain (\"industrial\", \"recipe\", \"scientific\").\n",
    "            query: The user query or rewritten query.\n",
    "            k: Number of results (defaults to config).\n",
    "            filters: Optional ChromaDB where-clause for metadata filtering.\n",
    "\n",
    "        Returns:\n",
    "            List of Documents with similarity_score in metadata.\n",
    "        \"\"\"\n",
    "        k = k or CONFIG.vector_store.search_top_k\n",
    "\n",
    "        logger.info(f\"Retrieving top-{k} from '{domain}' for: {query[:80]}...\")\n",
    "        docs = self.vector_store.search(domain, query, k=k, where=filters)\n",
    "        logger.info(\n",
    "            f\"Retrieved {len(docs)} documents \"\n",
    "            f\"(scores: {[f'{d.metadata.get('similarity_score', 0):.3f}' for d in docs]})\"\n",
    "        )\n",
    "        return docs\n",
    "\n",
    "    def retrieve_with_scores(\n",
    "        self,\n",
    "        domain: str,\n",
    "        query: str,\n",
    "        k: Optional[int] = None,\n",
    "    ) -> List[tuple]:\n",
    "        \"\"\"Retrieve documents as (Document, score) tuples.\"\"\"\n",
    "        docs = self.retrieve(domain, query, k=k)\n",
    "        return [\n",
    "            (doc, doc.metadata.get(\"similarity_score\", 0.0))\n",
    "            for doc in docs\n",
    "        ]\n",
    "''',\n",
    "\n",
    "}\n",
    "\n",
    "for rel_path, content in _FILES.items():\n",
    "    dst = os.path.join(PROJECT_ROOT, rel_path)\n",
    "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "    with open(dst, 'w') as f:\n",
    "        f.write(content)\n",
    "    print(f'  \\u2713 {rel_path}')\n",
    "\n",
    "print(f'\\nWrote {len(_FILES)} files to {PROJECT_ROOT}')\n",
    "\n",
    "for mod_name in list(sys.modules.keys()):\n",
    "    if any(pkg in mod_name for pkg in [\n",
    "        'config', 'foundation', 'rag_core',\n",
    "        'orchestration', 'agents', 'ingestion', 'evaluation',\n",
    "    ]):\n",
    "        del sys.modules[mod_name]\n",
    "\n",
    "from config.settings import CONFIG\n",
    "print(f'\\nConfig reloaded. LLM preset: {CONFIG.llm.preset}')\n",
    "print(f'Fallback preset: {CONFIG.llm.fallback_preset}')\n",
    "print(f'Embedding model: {CONFIG.embedding.model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KHco0kHFxK_4",
    "outputId": "803dbd8b-6d0f-49c8-f6e2-6b8913200e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: gemma3_instruct_12b\n",
      "Embeddings: Alibaba-NLP/gte-large-en-v1.5\n",
      "Domains: ['industrial', 'recipe', 'scientific']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from config.settings import CONFIG, LLMConfig\n",
    "from foundation.llm_wrapper import KerasHubChatModel\n",
    "from foundation.embedding_service import EmbeddingService\n",
    "from foundation.vector_store import VectorStoreService\n",
    "\n",
    "# Print configuration\n",
    "print(f\"LLM: {CONFIG.llm.preset}\")\n",
    "print(f\"Embeddings: {CONFIG.embedding.model_name}\")\n",
    "print(f\"Domains: {CONFIG.supervisor.domains}\")\n",
    "\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"roshcs\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"781502c08d14d778d3d23d4e58ca0751\"\n",
    "\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
    "# os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFv6CWdjZr3p",
    "outputId": "0b918478-a54e-482f-907f-c421e42bc1c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM loaded\n"
     ]
    }
   ],
   "source": [
    "# Now try loading\n",
    "from foundation.llm_wrapper import KerasHubChatModel\n",
    "from config.settings import CONFIG\n",
    "\n",
    "llm = KerasHubChatModel(config=CONFIG.llm)\n",
    "print(\"âœ… LLM loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346,
     "referenced_widgets": [
      "ea261468f2fb4e298e05bb849149ad9e",
      "9ab1fc07ce6742538a7081af05d0fd10",
      "354972c523ce4d25ae0b07045e6de40f",
      "68be00db64404c2e8a95f17102aa5f37",
      "93599041a32242bbb2527fc0a4b56c60",
      "d39eb82fa9de4a10af9aab0dadf236e0",
      "ad1ae9efd35a41d2bfabf40ad76d0766",
      "299165972c294cc68c7a8c307769ffd3",
      "bcd2d27bc236417194238612637ddeed",
      "d46f8e82073e459bbfbf80f6163c924b",
      "6fb0aaf970ba402db62d242c5b098c9e"
     ]
    },
    "id": "FAg2AkLMaEo9",
    "outputId": "5bf9e230-2832-46e3-86aa-7f392c9ecbd1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da5e08dac0545d7b4e76cd0b1631d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac56e5c2fcd4ecb8f91f5005ce9374f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5f754dd0054eaba57a9f4a10708ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2987789a37244bd5a416e91c06915644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749eb1f48b144db286638940a4372369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fa79bf87c24ddaa97aa227c83d74d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "ERROR:foundation.embedding_service:Failed to load embedding model: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'NewConfig {\n",
      "  \"architectures\": [\n",
      "    \"NewModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"Alibaba-NLP/new-impl--configuration.NewConfig\",\n",
      "    \"AutoModel\": \"Alibaba-NLP/new-impl--modeling.NewModel\",\n",
      "    \"AutoModelForMaskedLM\": \"Alibaba-NLP/new-impl--modeling.NewForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"Alibaba-NLP/new-impl--modeling.NewForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"Alibaba-NLP/new-impl--modeling.NewForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"Alibaba-NLP/new-impl--modeling.NewForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"Alibaba-NLP/new-impl--modeling.NewForTokenClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_norm_type\": \"layer_norm\",\n",
      "  \"logn_attention_clip1\": false,\n",
      "  \"logn_attention_scale\": false,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"new\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pack_qkv\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"rope\",\n",
      "  \"rope_parameters\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"ntk\"\n",
      "  },\n",
      "  \"rope_theta\": 160000,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unpad_inputs\": false,\n",
      "  \"use_memory_efficient_attention\": false,\n",
      "  \"vocab_size\": 30528\n",
      "}\n",
      "'. Use `repo_type` argument if needed.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'NewConfig {\n  \"architectures\": [\n    \"NewModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"Alibaba-NLP/new-impl--configuration.NewConfig\",\n    \"AutoModel\": \"Alibaba-NLP/new-impl--modeling.NewModel\",\n    \"AutoModelForMaskedLM\": \"Alibaba-NLP/new-impl--modeling.NewForMaskedLM\",\n    \"AutoModelForMultipleChoice\": \"Alibaba-NLP/new-impl--modeling.NewForMultipleChoice\",\n    \"AutoModelForQuestionAnswering\": \"Alibaba-NLP/new-impl--modeling.NewForQuestionAnswering\",\n    \"AutoModelForSequenceClassification\": \"Alibaba-NLP/new-impl--modeling.NewForSequenceClassification\",\n    \"AutoModelForTokenClassification\": \"Alibaba-NLP/new-impl--modeling.NewForTokenClassification\"\n  },\n  \"classifier_dropout\": null,\n  \"dtype\": \"float32\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"layer_norm_type\": \"layer_norm\",\n  \"logn_attention_clip1\": false,\n  \"logn_attention_scale\": false,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"new\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pack_qkv\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"rope\",\n  \"rope_parameters\": {\n    \"factor\": 2.0,\n    \"type\": \"ntk\"\n  },\n  \"rope_theta\": 160000,\n  \"transformers_version\": \"5.0.0\",\n  \"type_vocab_size\": 2,\n  \"unpad_inputs\": false,\n  \"use_memory_efficient_attention\": false,\n  \"vocab_size\": 30528\n}\n'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    420\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'NewConfig {\n  \"architectures\": [\n    \"NewModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"Alibaba-NLP/new-impl--configuration.NewConfig\",\n    \"AutoModel\": \"Alibaba-NLP/new-impl--modeling.NewModel\",\n    \"AutoModelForMaskedLM\": \"Alibaba-NLP/new-impl--modeling.NewForMaskedLM\",\n    \"AutoModelForMultipleChoice\": \"Alibaba-NLP/new-impl--modeling.NewForMultipleChoice\",\n    \"AutoModelForQuestionAnswering\": \"Alibaba-NLP/new-impl--modeling.NewForQuestionAnswering\",\n    \"AutoModelForSequenceClassification\": \"Alibaba-NLP/new-impl--modeling.NewForSequenceClassification\",\n    \"AutoModelForTokenClassification\": \"Alibaba-NLP/new-impl--modeling.NewForTokenClassification\"\n  },\n  \"classifier_dropout\": null,\n  \"dtype\": \"float32\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"layer_norm_type\": \"layer_norm\",\n  \"logn_attention_clip1\": false,\n  \"logn_attention_scale\": false,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"new\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pack_qkv\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"rope\",\n  \"rope_parameters\": {\n    \"factor\": 2.0,\n    \"type\": \"ntk\"\n  },\n  \"rope_theta\": 160000,\n  \"transformers_version\": \"5.0.0\",\n  \"type_vocab_size\": 2,\n  \"unpad_inputs\": false,\n  \"use_memory_efficient_attention\": false,\n  \"vocab_size\": 30528\n}\n'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-244218321.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfoundation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_service\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbeddingService\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingService\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLC fault code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/AI574_Multi_Domain_Agent/project/foundation/embedding_service.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/AI574_Multi_Domain_Agent/project/foundation/embedding_service.py\u001b[0m in \u001b[0;36m_load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading embedding model: {self.config.model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             self._model = SentenceTransformer(\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             ):\n\u001b[0;32m--> 327\u001b[0;31m                 modules, self.module_kwargs = self._load_sbert_model(\n\u001b[0m\u001b[1;32m    328\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m_load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   2303\u001b[0m                 \u001b[0;31m# Newer modules that support the new loading method are loaded with the new style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2304\u001b[0m                 \u001b[0;31m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2305\u001b[0;31m                 module = module_class.load(\n\u001b[0m\u001b[1;32m   2306\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2307\u001b[0m                     \u001b[0;31m# Loading-specific keyword arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         )\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_peft_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Get the signature of the auto_model's forward method to pass only the expected arguments from `features`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m_load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_mt5_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 self.auto_model = AutoModel.from_pretrained(\n\u001b[0m\u001b[1;32m    198\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_for_auto_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_generation_mixin_to_remote_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3987\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3988\u001b[0;31m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[1;32m   3989\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3990\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"revision\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_config_key\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_config_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_config_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    625\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m             ) from e\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{e}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'NewConfig {\n  \"architectures\": [\n    \"NewModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"Alibaba-NLP/new-impl--configuration.NewConfig\",\n    \"AutoModel\": \"Alibaba-NLP/new-impl--modeling.NewModel\",\n    \"AutoModelForMaskedLM\": \"Alibaba-NLP/new-impl--modeling.NewForMaskedLM\",\n    \"AutoModelForMultipleChoice\": \"Alibaba-NLP/new-impl--modeling.NewForMultipleChoice\",\n    \"AutoModelForQuestionAnswering\": \"Alibaba-NLP/new-impl--modeling.NewForQuestionAnswering\",\n    \"AutoModelForSequenceClassification\": \"Alibaba-NLP/new-impl--modeling.NewForSequenceClassification\",\n    \"AutoModelForTokenClassification\": \"Alibaba-NLP/new-impl--modeling.NewForTokenClassification\"\n  },\n  \"classifier_dropout\": null,\n  \"dtype\": \"float32\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"layer_norm_type\": \"layer_norm\",\n  \"logn_attention_clip1\": false,\n  \"logn_attention_scale\": false,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"new\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pack_qkv\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"rope\",\n  \"rope_parameters\": {\n    \"factor\": 2.0,\n    \"type\": \"ntk\"\n  },\n  \"rope_theta\": 160000,\n  \"transformers_version\": \"5.0.0\",\n  \"type_vocab_size\": 2,\n  \"unpad_inputs\": false,\n  \"use_memory_efficient_attention\": false,\n  \"vocab_size\": 30528\n}\n'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "from foundation.embedding_service import EmbeddingService\n",
    "\n",
    "embedder = EmbeddingService()\n",
    "\n",
    "test_vec = embedder.embed_query(\"PLC fault code\")\n",
    "print(f\"âœ… Embedding dimension: {len(test_vec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sf66QNfkgpau",
    "outputId": "b02f0805-0d8c-443d-c75d-83eab2f0dd8e"
   },
   "outputs": [],
   "source": [
    "from foundation.vector_store import VectorStoreService\n",
    "\n",
    "vs = VectorStoreService(embedding_service=embedder)\n",
    "print(\"âœ… Vector store initialized\")\n",
    "print(vs.get_all_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0D6UJ9FGhKqk",
    "outputId": "42e46ac2-0a18-454b-aca4-4ca250a92853"
   },
   "outputs": [],
   "source": [
    "from ingestion.index_builder import IndexBuilder\n",
    "from ingestion.document_loader import DocumentLoader\n",
    "from ingestion.chunking_pipeline import ChunkingPipeline\n",
    "\n",
    "builder = IndexBuilder(vector_store=vs)\n",
    "\n",
    "# â”€â”€ Industrial: Rockwell Automation / Allen-Bradley â”€â”€\n",
    "sample_industrial = [\n",
    "    \"\"\"Fault Code F002 - Allen-Bradley PowerFlex 525 VFD: Auxiliary Input.\n",
    "    Description: An external fault has been detected via the auxiliary input terminal.\n",
    "    Possible Causes:\n",
    "    1. External device connected to auxiliary input is reporting a fault condition\n",
    "    2. Wiring issue on the auxiliary digital input terminal\n",
    "    3. Incorrect parameter configuration for the auxiliary input function\n",
    "    Troubleshooting Steps:\n",
    "    1. Check the external device connected to the auxiliary input for faults\n",
    "    2. Verify wiring connections on terminal block TB2\n",
    "    3. Check parameter A070 [Fault Config 2] for correct auxiliary input configuration\n",
    "    4. Inspect for damaged or loose wiring on the digital input terminals\n",
    "    Resolution: Clear the external fault condition and reset the drive by cycling the Enable input or pressing the Stop/Reset key on the HIM.\"\"\",\n",
    "\n",
    "    \"\"\"Fault Code F004 - Allen-Bradley PowerFlex 525 VFD: Undervoltage.\n",
    "    Description: The DC bus voltage has dropped below the undervoltage trip level.\n",
    "    Possible Causes:\n",
    "    1. Input power supply voltage too low or momentary power dip\n",
    "    2. Incoming power supply fuses blown or circuit breaker tripped\n",
    "    3. Loose connections on input power terminals R/L1, S/L2, T/L3\n",
    "    4. DC bus capacitors degraded (check capacitor health indicator LED)\n",
    "    Troubleshooting Steps:\n",
    "    1. Measure incoming line voltage at drive terminals - must be within nameplate rating +/-10%\n",
    "    2. Check all three phases for voltage balance (max 3% imbalance)\n",
    "    3. Inspect and tighten input power terminal connections (torque to 1.4 Nm)\n",
    "    4. Check parameter A531 [DC Bus Voltage] for current reading\n",
    "    5. Review fault log in parameter A700-A706 for fault history and timestamps\n",
    "    Resolution: Restore proper input voltage. If capacitors are degraded, replace the drive.\n",
    "    SAFETY: Disconnect and lockout/tagout all power sources before inspecting terminals.\n",
    "    Wait 5 minutes for DC bus capacitors to discharge before servicing.\"\"\",\n",
    "\n",
    "    \"\"\"Fault Code F005 - Allen-Bradley PowerFlex 525 VFD: Overvoltage.\n",
    "    Description: The DC bus voltage has exceeded the overvoltage trip level.\n",
    "    Possible Causes:\n",
    "    1. Input line voltage exceeds drive nameplate rating\n",
    "    2. Excessive regenerative energy from motor deceleration (overhauling load)\n",
    "    3. Deceleration time too short for the load inertia\n",
    "    4. Dynamic brake resistor circuit failure or incorrect sizing\n",
    "    Troubleshooting Steps:\n",
    "    1. Measure input voltage - must not exceed 528 VAC for 480V class drives\n",
    "    2. Increase deceleration time in parameter A092 [Decel Time 1]\n",
    "    3. Enable bus regulator: set parameter A540 [Bus Reg Mode] to option 1 (Enabled)\n",
    "    4. If using dynamic braking, check DB resistor connections and resistance value\n",
    "    5. Check parameter A531 [DC Bus Voltage] - nominal is ~650V for 480V input\n",
    "    Resolution: Reduce input voltage or extend deceleration time. Add DB resistor for overhauling loads.\"\"\",\n",
    "\n",
    "    \"\"\"Allen-Bradley CompactLogix 5380 Controller - EtherNet/IP Configuration Guide.\n",
    "    Model: 1769-L33ER CompactLogix 5380\n",
    "    Prerequisites: RSLogix 5000 v32+ or Studio 5000 Logix Designer v32+\n",
    "    Step 1: Create new project - select 1769-L33ER controller, revision 32+\n",
    "    Step 2: Configure controller IP address via USB connection using BOOTP/DHCP tool\n",
    "    Step 3: Add EtherNet/IP module in I/O Configuration tree\n",
    "    Step 4: Right-click controller > Properties > set IP address (e.g., 192.168.1.10)\n",
    "    Step 5: Add remote I/O devices - right-click EtherNet/IP > New Module\n",
    "    Step 6: Configure Produced/Consumed tags for controller-to-controller communication\n",
    "    Step 7: Download program and verify connection indicators:\n",
    "      - OK LED: Solid green = running, Flashing green = program mode\n",
    "      - ENET LED: Solid green = has connections, Flashing green = no connections\n",
    "    Common Issue: If ENET LED is off, check IP config and network switch connection.\n",
    "    Use RSLinx Classic > RSWho to verify controller appears on EtherNet/IP network.\"\"\",\n",
    "\n",
    "    \"\"\"Allen-Bradley PanelView Plus 7 HMI - Troubleshooting Communication Failures.\n",
    "    Symptom: PanelView displays \"Controller not found\" or shows stale data.\n",
    "    Possible Causes:\n",
    "    1. Incorrect IP address or subnet mask configuration\n",
    "    2. EtherNet/IP cable disconnected or faulty\n",
    "    3. RSLinx Enterprise communication path misconfigured\n",
    "    4. Controller is in Program mode instead of Run mode\n",
    "    Troubleshooting Steps:\n",
    "    1. On PanelView: press Terminal Settings > Networks > verify IP address\n",
    "    2. Ping the controller IP from PanelView diagnostics screen\n",
    "    3. In FactoryTalk View Studio: check Communication Setup > verify shortcut path\n",
    "    4. Ensure controller and HMI are on same subnet (e.g., both 192.168.1.x/24)\n",
    "    5. Check Ethernet cable and switch port LEDs for link activity\n",
    "    6. Review controller mode - must be in Run or Remote Run for live data\n",
    "    Resolution: Correct IP addressing and verify physical network connectivity.\n",
    "    If using managed switch, verify VLANs are not isolating devices.\"\"\",\n",
    "\n",
    "    \"\"\"Preventive Maintenance Schedule - Allen-Bradley PowerFlex 755 Drive System.\n",
    "    Weekly: Check drive status LEDs and HIM display for active alarms\n",
    "    Monthly: Inspect cooling fans for proper operation, clean air filters\n",
    "    Quarterly: Verify DC bus voltage (parameter 15 - Bus Voltage), check capacitor\n",
    "    formation indicator, tighten all power and control terminal connections\n",
    "    Semi-Annually: Thermal scan of power connections, verify ground fault monitoring,\n",
    "    backup drive parameters using Connected Components Workbench (CCW)\n",
    "    Annually: Full parameter backup, firmware version audit against Rockwell\n",
    "    compatibility matrix, capacitor health test, clean power module heat sinks\n",
    "    Every 5 Years: Replace cooling fans (recommended life), capacitor reformation\n",
    "    MTBF: PowerFlex 755 rated at approximately 200,000 hours (28+ years) at 40C ambient.\n",
    "    Critical Spares to Stock: Cooling fan assembly (KIT-F755FAN), control board fuse,\n",
    "    fiber optic cables for multi-axis configurations.\"\"\",\n",
    "]\n",
    "\n",
    "count = builder.index_industrial_texts(sample_industrial, source=\"rockwell_automation\")\n",
    "print(f\"âœ… Indexed {count} industrial chunks\")\n",
    "\n",
    "# â”€â”€ Recipe sample data â”€â”€\n",
    "sample_recipes = [\n",
    "    \"\"\"Recipe: Classic Chocolate Chip Cookies\n",
    "    Prep Time: 45 minutes. Ingredients: 2 1/4 cups flour, 1 tsp baking soda,\n",
    "    1 tsp salt, 1 cup butter softened, 3/4 cup sugar, 3/4 cup brown sugar,\n",
    "    2 large eggs, 2 tsp vanilla, 2 cups chocolate chips.\n",
    "    Egg Substitutes: Use 1/4 cup unsweetened applesauce per egg,\n",
    "    or 1 mashed banana per egg, or 3 tbsp aquafaba per egg.\"\"\",\n",
    "\n",
    "    \"\"\"Recipe: Quick Chicken Stir-Fry\n",
    "    Prep Time: 20 minutes. Ingredients: 1 lb chicken breast, 2 bell peppers,\n",
    "    1 cup rice, 3 tbsp soy sauce, 1 tbsp sesame oil, 2 cloves garlic, ginger.\n",
    "    Steps: Cook rice. Slice chicken. Heat oil in wok over high heat.\n",
    "    Stir-fry chicken 5-6 min. Add vegetables and garlic. Add soy sauce.\n",
    "    Nutrition: ~450 calories per serving, 35g protein.\"\"\",\n",
    "\n",
    "    \"\"\"Recipe: Homemade Margherita Pizza\n",
    "    Prep Time: 90 minutes (including dough rise). Ingredients: 3 cups flour,\n",
    "    1 packet yeast, 1 cup warm water, 2 tbsp olive oil, 1 tsp salt, 1 tsp sugar,\n",
    "    1 cup crushed San Marzano tomatoes, 8 oz fresh mozzarella, fresh basil.\n",
    "    Steps: Mix dough, let rise 1 hour. Preheat oven to 475F. Stretch dough,\n",
    "    add sauce and cheese. Bake 12-15 min until crust is golden.\n",
    "    Gluten-Free Alternative: Substitute 3 cups gluten-free flour blend plus\n",
    "    1 tsp xanthan gum for regular flour.\"\"\",\n",
    "]\n",
    "\n",
    "recipe_docs = DocumentLoader.from_texts(sample_recipes, \"recipe\", \"demo_recipes\")\n",
    "chunker = ChunkingPipeline()\n",
    "recipe_chunks = chunker.chunk_documents(recipe_docs, domain=\"recipe\")\n",
    "count = vs.add_documents(\"recipe\", recipe_chunks)\n",
    "print(f\"âœ… Indexed {count} recipe chunks\")\n",
    "\n",
    "# â”€â”€ Scientific sample data â”€â”€\n",
    "sample_scientific = [\n",
    "    \"\"\"Title: Attention Is All You Need\n",
    "    Authors: Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin\n",
    "    Abstract: We propose a new network architecture, the Transformer, based solely\n",
    "    on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "    The Transformer allows for significantly more parallelization and achieves\n",
    "    new state of the art in translation quality. ArXiv ID: 1706.03762\"\"\",\n",
    "\n",
    "    \"\"\"Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
    "    Authors: Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, Kuttler, Lewis, Yih, Rocktaschel, Riedel, Kiela\n",
    "    Abstract: We explore a general-purpose fine-tuning recipe for retrieval-augmented\n",
    "    generation (RAG) which combines pre-trained parametric and non-parametric memory\n",
    "    for language generation. ArXiv ID: 2005.11401\"\"\",\n",
    "\n",
    "    \"\"\"Title: Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n",
    "    Authors: Asai, Wu, Wang, Sil, Hajishirzi\n",
    "    Abstract: We introduce Self-RAG, a framework that trains a single LM to adaptively\n",
    "    retrieve passages on demand, generate text informed by retrieved passages, and\n",
    "    critique its own output using special reflection tokens. Self-RAG significantly\n",
    "    outperforms existing RAG approaches and LLMs on multiple tasks. ArXiv ID: 2310.11511\"\"\",\n",
    "]\n",
    "\n",
    "sci_docs = DocumentLoader.from_texts(sample_scientific, \"scientific\", \"demo_papers\")\n",
    "sci_chunks = chunker.chunk_documents(sci_docs, domain=\"scientific\")\n",
    "count = vs.add_documents(\"scientific\", sci_chunks)\n",
    "print(f\"âœ… Indexed {count} scientific chunks\")\n",
    "\n",
    "# Status check\n",
    "print(\"\\nğŸ“Š Index Status:\")\n",
    "for stat in vs.get_all_stats():\n",
    "    print(f\"  {stat['domain']:>12}: {stat['document_count']} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DQiyqf5msKV",
    "outputId": "241033b8-5c83-41ee-fc34-cd133387b4e8"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Make sure the project root is at the FRONT of the path\n",
    "PROJECT = '/content/drive/MyDrive/AI574_Multi_Domain_Agent/project'\n",
    "if PROJECT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT)\n",
    "\n",
    "# Verify the right modules are found\n",
    "import agents.industrial_agent\n",
    "print(f\"âœ… Found: {agents.industrial_agent.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CHfDt8Eke6y",
    "outputId": "ca247829-a925-460f-9ed2-2ad0ebca6453"
   },
   "outputs": [],
   "source": [
    "import importlib, orchestration.workflow_graph, orchestration.state_schema\n",
    "importlib.reload(orchestration.state_schema)\n",
    "importlib.reload(orchestration.workflow_graph)\n",
    "from orchestration.workflow_graph import build_workflow, run_query\n",
    "\n",
    "try:\n",
    "    llm, vs, builder\n",
    "except NameError:\n",
    "    raise RuntimeError(\n",
    "        \"Run the cells above first so 'llm', 'vs', and 'builder' exist: \"\n",
    "        \"load LLM, create embedder & vector store, run IndexBuilder and index data.\"\n",
    "    )\n",
    "\n",
    "workflow = build_workflow(llm=llm, vector_store=vs, index_builder=builder)\n",
    "print(\"âœ… Workflow compiled!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(result):\n",
    "    \"\"\"Pretty-print query result with visual timing breakdown.\"\"\"\n",
    "    domain = result.get(\"domain\", \"?\")\n",
    "    conf   = result.get(\"confidence\", 0)\n",
    "    status = result.get(\"status\", \"?\")\n",
    "    esc    = result.get(\"escalated\", False)\n",
    "    srcs   = result.get(\"sources\", [])\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Domain: {domain:<14} Confidence: {conf:.2f}\")\n",
    "    print(f\"  Status: {status:<14} Escalated:  {esc}\")\n",
    "    print(f\"  Sources: {len(srcs)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\n{result.get('response', '')}\")\n",
    "\n",
    "    # â”€â”€ Timing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    t = result.get(\"timing\")\n",
    "    if not t:\n",
    "        print(\"\\n  (no timing data â€” run sync cell to update workflow_graph.py)\")\n",
    "        return\n",
    "\n",
    "    total = t.get(\"total_s\", 0) or 0.001\n",
    "    sup   = t.get(\"supervisor_s\", 0)\n",
    "    agent = t.get(\"agent_s\", 0)\n",
    "    c     = t.get(\"crag\") or {}\n",
    "\n",
    "    retrieve = c.get(\"retrieve_s\", 0)\n",
    "    grade    = c.get(\"grade_s\", 0)\n",
    "    rewrite  = c.get(\"rewrite_s\", 0)\n",
    "    generate = c.get(\"generate_s\", 0)\n",
    "    validate = c.get(\"validate_s\", 0)\n",
    "    overhead = max(0, agent - (retrieve + grade + rewrite + generate + validate))\n",
    "\n",
    "    steps = [\n",
    "        (\"Supervisor\", sup),\n",
    "        (\"  Retrieve\", retrieve),\n",
    "        (\"  Grade\",    grade),\n",
    "        (\"  Rewrite\",  rewrite),\n",
    "        (\"  Generate\", generate),\n",
    "        (\"  Validate\", validate),\n",
    "        (\"  Overhead\", overhead),\n",
    "    ]\n",
    "\n",
    "    BAR = 25\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(f\"  TIMING BREAKDOWN  (total {total:.1f}s / {total/60:.2f} min)\")\n",
    "    print(f\"{'â”€'*60}\")\n",
    "    for label, s in steps:\n",
    "        if s < 0.01 and label == \"  Rewrite\":\n",
    "            continue\n",
    "        pct  = (s / total) * 100\n",
    "        fill = int(BAR * s / total)\n",
    "        bar  = \"â–ˆ\" * fill + \"â–‘\" * (BAR - fill)\n",
    "        print(f\"  {label:<12} {bar} {s:5.1f}s  ({pct:4.1f}%)\")\n",
    "    print(f\"{'â”€'*60}\")\n",
    "    print(f\"  {'Total':<12} {'':>{BAR}} {total:5.1f}s\")\n",
    "    print(f\"{'â”€'*60}\")\n",
    "\n",
    "# â”€â”€ Run test query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "result = run_query(workflow, \"My PowerFlex 525 drive is showing fault F004, how do I fix it?\")\n",
    "display_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rrmjqQeqKww",
    "outputId": "6003ac82-629e-463e-f6de-7cdada89bd55"
   },
   "outputs": [],
   "source": [
    "# Test: Recipe query\n",
    "result = run_query(workflow, \"What can I substitute for eggs in cookies?\")\n",
    "display_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cg5BM0HpsuDR",
    "outputId": "20661136-a666-4a26-e80f-ec99a9137b83"
   },
   "outputs": [],
   "source": [
    "# Test: Scientific query\n",
    "result = run_query(workflow, \"Summarize the key ideas behind RAG in NLP\")\n",
    "display_result(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5w8knd59svrw",
    "outputId": "09fb5722-193c-4ae2-b546-be72cadd07df"
   },
   "outputs": [],
   "source": [
    "# Test: Ambiguous query (should trigger clarification)\n",
    "result = run_query(workflow, \"What temperature should I set to avoid failure?\")\n",
    "display_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does one query take 3+ minutes? / LLM throughput on A100\n",
    "\n",
    "**Reasons:**\n",
    "1. **Multiple LLM calls per query** â€” Supervisor (route) + batch grader + response generator + hallucination check = **4 sequential calls**. Each call is a full 9B forward/generation.\n",
    "2. **Keras/JAX is not an inference-optimized stack** â€” Frameworks like TensorRT-LLM or vLLM get **~50â€“60 tok/s** for 8â€“10B on A100. Keras Hub on JAX typically does **~5â€“20 tok/s** (no fused kernels, no continuous batching).\n",
    "3. **Rough token budget per query** â€” ~80 (supervisor) + ~300 (grader) + ~400â€“800 (generator) + ~80 (hallucination) â‰ˆ **900â€“1300 output tokens**. At 10 tok/s thatâ€™s 90â€“130 s just for generation; add overhead and you get 3+ min.\n",
    "\n",
    "**To see your actual throughput:** run the cell below and check the log lines `LLM call: Xs | ~Y output tokens | ~Z tok/s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable INFO logging so \"LLM call: ... tok/s\" lines appear\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "for _ in [\"foundation.llm_wrapper\", \"rag_core.crag_pipeline\"]:\n",
    "    logging.getLogger(_).setLevel(logging.INFO)\n",
    "\n",
    "# Timed re-run with full breakdown\n",
    "result = run_query(workflow, \"My PowerFlex 525 drive is showing fault F004, how do I fix it?\")\n",
    "display_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSK54iS27IDT"
   },
   "source": [
    "# New Section"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "299165972c294cc68c7a8c307769ffd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "354972c523ce4d25ae0b07045e6de40f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_299165972c294cc68c7a8c307769ffd3",
      "max": 391,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bcd2d27bc236417194238612637ddeed",
      "value": 391
     }
    },
    "68be00db64404c2e8a95f17102aa5f37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d46f8e82073e459bbfbf80f6163c924b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6fb0aaf970ba402db62d242c5b098c9e",
      "value": "â€‡391/391â€‡[00:00&lt;00:00,â€‡1357.94it/s,â€‡Materializingâ€‡param=pooler.dense.weight]"
     }
    },
    "6fb0aaf970ba402db62d242c5b098c9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93599041a32242bbb2527fc0a4b56c60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ab1fc07ce6742538a7081af05d0fd10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d39eb82fa9de4a10af9aab0dadf236e0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ad1ae9efd35a41d2bfabf40ad76d0766",
      "value": "Loadingâ€‡weights:â€‡100%"
     }
    },
    "ad1ae9efd35a41d2bfabf40ad76d0766": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bcd2d27bc236417194238612637ddeed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d39eb82fa9de4a10af9aab0dadf236e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d46f8e82073e459bbfbf80f6163c924b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea261468f2fb4e298e05bb849149ad9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ab1fc07ce6742538a7081af05d0fd10",
       "IPY_MODEL_354972c523ce4d25ae0b07045e6de40f",
       "IPY_MODEL_68be00db64404c2e8a95f17102aa5f37"
      ],
      "layout": "IPY_MODEL_93599041a32242bbb2527fc0a4b56c60"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
